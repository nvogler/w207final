{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we examine a dataset containing crime statistics in the San Francisco area. The data is analyzed from  analyzed with the goal of creating a model to predict the nature of a crime based only on its time and location. \n",
    "\n",
    "More information on the project and its dataset can be found here:\n",
    "https://www.kaggle.com/c/sf-crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we import the needed packages for python...\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "from scipy import stats, integrate\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as holidaysCalendar\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pandas.options.display.max_columns = 200\n",
    "pandas.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header (from sample submission)- these are the categories of crime we must predict. \n",
    "header = [\"ID\", \"ARSON\", \"ASSAULT\", \"BAD CHECKS\", \"BRIBERY\", \"BURGLARY\",\n",
    "          \"DISORDERLY CONDUCT\", \"DRIVING UNDER THE INFLUENCE\", \"DRUG/NARCOTIC\",\n",
    "          \"DRUNKENNESS\", \"EMBEZZLEMENT\", \"EXTORTION\", \"FAMILY OFFENSES\",\n",
    "          \"FORGERY/COUNTERFEITING\", \"FRAUD\", \"GAMBLING\", \"KIDNAPPING\", \"LARCENY/THEFT\",\n",
    "          \"LIQUOR LAWS\", \"LOITERING\", \"MISSING PERSON\", \"NON-CRIMINAL\", \"OTHER OFFENSES\",\n",
    "          \"PORNOGRAPHY/OBSCENE MAT\", \"PROSTITUTION\", \"RECOVERED VEHICLE\", \"ROBBERY\",\n",
    "          \"RUNAWAY\", \"SECONDARY CODES\", \"SEX OFFENSES FORCIBLE\", \"SEX OFFENSES NON FORCIBLE\",\n",
    "          \"STOLEN PROPERTY\", \"SUICIDE\", \"SUSPICIOUS OCC\", \"TREA\", \"TRESPASS\", \"VANDALISM\",\n",
    "          \"VEHICLE THEFT\", \"WARRANTS\", \"WEAPON LAWS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions\n",
    "we define these functions to help parse and add meaning to the data. This is particularly\n",
    "important because much of the data are strings.\n",
    "\n",
    "They include-  \n",
    "--1) Using the time of day to create buckets- ie- 0800= morning  \n",
    "--2) Whether the location of a crime was on a street corner or in the middle of a block  \n",
    "--3) Whether the crime (roughly) occured during daytime or nighttime (day defined as after 7am and before 7pm)  \n",
    "--4) Using the supplied latitude/longitude to determine the crime's zip code  \n",
    "--5) Whether the crime occured on a holiday  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveResults(result, outputname):\n",
    "    # Output, format\n",
    "    ## Add rows for dummy creation for all possible output labels \n",
    "    result = np.append(result, list(range(39)))\n",
    "    ## Convert to pandas data type and create dummies\n",
    "    result = pandas.DataFrame(result)\n",
    "    result = pandas.get_dummies(result[0], prefix='', prefix_sep='')\n",
    "    ## Remove post-prediction added rows\n",
    "    result = result[:-39]\n",
    "    ## Save to CSV\n",
    "    result.to_csv(outputname, compression='gzip', chunksize=1000)\n",
    "\n",
    "def DetermineTime(hour):\n",
    "    ## Add early morning, morning, afternoon, early evening, evening, late evening\n",
    "    ## 0500-0800, 0800-1100, 1100-1600, 1600-2100, 2100-0200, 0200-0500\n",
    "    if hour <= 2:\n",
    "        return \"evening\"\n",
    "    elif hour <= 5:\n",
    "        return \"lateE\"\n",
    "    elif hour <= 8:\n",
    "        return \"earlyM\"\n",
    "    elif hour <= 11:\n",
    "        return \"morning\"\n",
    "    elif hour <= 16:\n",
    "        return \"afternoon\"\n",
    "    elif hour <= 21:\n",
    "        return \"earlyE\"\n",
    "    else:\n",
    "        return \"evening\"\n",
    "\n",
    "def DetermineLocationType(address):\n",
    "    ##Add block or corner\n",
    "    if \"block\" in address.lower():\n",
    "        return \"block\"\n",
    "    elif \"/\" in address:\n",
    "        return \"corner\"\n",
    "    return None\n",
    "\n",
    "def DetermineDayTime(time):\n",
    "    ##Add Day Time\n",
    "    time = int(time)\n",
    "    if time >= 7 and time <= 19:\n",
    "        return True\n",
    "    elif time < 7 or time >= 19:\n",
    "        return False\n",
    "    return None\n",
    "\n",
    "def Normalize(X):\n",
    "    minx = min(X)\n",
    "    maxx = max(X)\n",
    "    subx = max(X)-min(X)\n",
    "    return X.apply(lambda x: (x-minx)/subx)\n",
    "\n",
    "def DetermineBlockCol(datacol):\n",
    "    return 10 * round(datacol, 1)\n",
    " \n",
    "def DetermineBlockRow(datarow):\n",
    "    return 100 * round(datarow, 1)\n",
    "    \n",
    "def DetermineBlock(data):\n",
    "    return int(xrow) + int(yrow)\n",
    "\n",
    "def DetermineUnemp(data, unemp_dict):\n",
    "    return unemp_dict[str(data.year)][str(calendar.month_abbr[data.month])]\n",
    "\n",
    "def GenerateCoordToZipMap(filename, dict, data, read, write):\n",
    "    # Load coords\n",
    "    coords = data[['X', 'Y']]\n",
    "    \n",
    "    # Create/load dict\n",
    "    geo_dict = dict\n",
    "    if read:\n",
    "        # Load GeoDict from file\n",
    "        with open(datafile, 'r') as file:\n",
    "            geo_dict = json.load(file)\n",
    "    \n",
    "    # Generate mappings\n",
    "    for coord in coords.iterrows():\n",
    "        X = round(coord[1][1], 4)\n",
    "        Y = round(coord[1][0], 4)\n",
    "        xy = str(X) + \", \" + str(Y)\n",
    "        # Check if existing\n",
    "        if xy in geo_dict.keys():\n",
    "            print (\"Found! \" + str(geo_dict[xy]) + \" X: \" + str(X) + \"  Y: \" + str(Y))\n",
    "        else:\n",
    "            success = False\n",
    "            while not success:\n",
    "                try:\n",
    "                    location = geolocator.reverse(xy)\n",
    "                    zip = location.address.split(',')[-2].strip()\n",
    "                    geo_dict[xy] = zip\n",
    "                    print (\"Added: \" + str(zip) + \"  X: \" + str(X) + \"  Y: \" + str(Y))\n",
    "                    success = True\n",
    "                except:\n",
    "                    time.sleep(5)\n",
    "    if write:\n",
    "        # Write GeoDict to file\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write(json.dumps(geo_dict))\n",
    "    \n",
    "    return geo_dict\n",
    "\n",
    "def CoordsToZip(geo_dict, X, Y):\n",
    "    X = round(X, 4)\n",
    "    Y = round(Y, 4)\n",
    "    xy = str(X) + \", \" + str(Y)\n",
    "    if xy in geo_dict.keys():\n",
    "        return geo_dict[xy]\n",
    "    # Not found\n",
    "    return \"00000\"\n",
    "\n",
    "def DetermineHoliday(date, holidays):\n",
    "    if date in holidays:\n",
    "        return 1\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data\n",
    "Here we load the data provided by Kaggle. We haven't made any transformations yet. \n",
    "The train data has 878,048 crimes, the test data has 884,262 crimes. \n",
    "Note that the test data does not include the crime's category (which we are predicting) or its description, \n",
    "so we do not want to train the model on these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pandas.read_csv(\"train.csv\")\n",
    "df_test = pandas.read_csv(\"test.csv\")\n",
    "\n",
    "# Drop duplicates\n",
    "#df_train = df_train.drop_duplicates()\n",
    "#df_test = df_test.drop_duplicates()\n",
    "\n",
    "# Shuffle\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print (df_train.shape)\n",
    "print (df_test.shape)\n",
    "print (df_train.head())\n",
    "print (df_test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Additions- Unemployment and weather\n",
    "Here we wish to pull in weather and unemployment data, which we believe will be important in determining crime. \n",
    "We pulled in publicly available data which can be matched to the date of the crime to add the max temp and total\n",
    "precipitation of the day of the crime and the unemployment rate\n",
    "\n",
    "Note that the weather is from the SFO Airport station, and unemployment is for the whole state of CA, and is monthly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we maniuplate a few columns in the train data- the DATE column is formatted to match the weather data,\n",
    "# and the YR-MTH COLUMN to match to the unemployment data\n",
    "df_train[\"DATE\"]=df_train.Dates.str.split('\\s+').str[0]\n",
    "df_train['YR-MTH']=df_train['DATE'].map(lambda x: x[:7])\n",
    "\n",
    "print (df_train.shape)\n",
    "print (df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we pull in the unemployment data. It was sourced from Quandl.\n",
    "# We then maniuplate the date column to match with the YR-MNTH column in the train data\n",
    "unemployment=pandas.read_csv(\"https://www.quandl.com/api/v3/datasets/FRED/CAUR.csv?api_key=EaN-sDKDUb9c3AXGdQkS\")\n",
    "unemployment['YR-MTH']=unemployment['Date'].map(lambda x: x[:7])\n",
    "\n",
    "print (unemployment.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we merge the train data with the unemployment data on the common YR-MTH column. \n",
    "# It is a left outer join to not disturb the integrity of the train data otherwise \n",
    "df_train=pandas.merge(df_train, unemployment, on ='YR-MTH', how ='left')\n",
    "df_train=df_train.drop('Date', 1)\n",
    "\n",
    "print (df_train.shape)\n",
    "print (df_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load in the weather data from the SFO airport weather station\n",
    "# We pull the total precipitation for the day, the maximum high temperature, and add a new column, whether precipition > 0\n",
    "df_weather=pandas.read_csv(\"climatedata.csv\")\n",
    "df_weather=df_weather[df_weather.STATION=='USW00023234']\n",
    "df_weather['DID_RAIN']=np.where(df_weather.PRCP >0,1,0)\n",
    "df_weather=df_weather[['DATE','PRCP','TMAX','DID_RAIN']]\n",
    "\n",
    "print (df_weather.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we merge the train and weather data, using a left outer join on as we did previously to the unemployment data\n",
    "df_train=pandas.merge(df_train, df_weather, on ='DATE', how ='left')\n",
    "\n",
    "print (df_train.shape)\n",
    "print (df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the same process for the test data, adding both unemployment and weather data:\n",
    "df_test[\"DATE\"]=df_test.Dates.str.split('\\s+').str[0]\n",
    "df_test['YR-MTH']=df_test['DATE'].map(lambda x: x[:7])\n",
    "\n",
    "df_test=pandas.merge(df_test, unemployment, on ='YR-MTH', how ='left')\n",
    "df_test=df_test.drop('Date', 1)\n",
    "\n",
    "df_test=pandas.merge(df_test, df_weather, on ='DATE', how ='left')\n",
    "\n",
    "print (df_test.shape)\n",
    "print (df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Adding:\n",
    "Here we run the functions defined above to add features to the data based on the original data provided.\n",
    "\n",
    "We do this for both test and train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Dates\n",
    "## Hour of day\n",
    "df_train['Hour'] = pandas.to_datetime(df_train['Dates']).dt.hour\n",
    "df_test['Hour'] = pandas.to_datetime(df_test['Dates']).dt.hour\n",
    "\n",
    "## Time of day\n",
    "df_train['Time'] = df_train['Hour'].apply(lambda x: DetermineTime(x))\n",
    "df_test['Time'] = df_test['Hour'].apply(lambda x: DetermineTime(x))\n",
    "\n",
    "## Day\n",
    "df_train['DayTime'] = df_train['Hour'].apply(lambda x: DetermineDayTime(x))\n",
    "df_test['DayTime'] = df_test['Hour'].apply(lambda x: DetermineDayTime(x))\n",
    "\n",
    "## Week of year\n",
    "df_train['Week'] = pandas.to_datetime(df_train['Dates']).dt.week\n",
    "df_test['Week'] = pandas.to_datetime(df_test['Dates']).dt.week\n",
    "\n",
    "## Season of year\n",
    "def DetermineSeason(week):\n",
    "    if (week >= 47) or (week <=7 ):\n",
    "        return \"Winter\"\n",
    "    if (week >= 8) or (week <=20 ):\n",
    "        return \"Spring\"\n",
    "    if (week >= 21) or (week <=33 ):\n",
    "        return \"Summer\"\n",
    "    if (week >= 34) or (week <=46 ):\n",
    "        return \"Autumn\"\n",
    "    return \"Error\"\n",
    "\n",
    "df_train['Season'] = df_train['Week'].apply(lambda x: DetermineSeason(x))\n",
    "df_test['Season'] = df_test['Week'].apply(lambda x: DetermineSeason(x))\n",
    "\n",
    "## Reduce to date\n",
    "df_train['Date'] = pandas.to_datetime(df_train['Dates']).dt.date\n",
    "df_test['Date'] = pandas.to_datetime(df_test['Dates']).dt.date\n",
    "\n",
    "# Adjust out of bounds values\n",
    "median_x = np.median(df_train['X'])\n",
    "median_y = np.median(df_train['Y'])\n",
    "def AssignMedianX(x, median_x):\n",
    "    if x == -120.5:\n",
    "        return median_x\n",
    "    else:\n",
    "        return x\n",
    "def AssignMedianY(y, median_y):\n",
    "    if y > 38:\n",
    "        return median_y\n",
    "    else:\n",
    "        return y\n",
    "\n",
    "df_train['X'] = df_train['X'].apply(lambda x: AssignMedianX(x, median_x))\n",
    "df_train['Y'] = df_train['Y'].apply(lambda x: AssignMedianY(x, median_y))\n",
    "df_test['X'] = df_test['X'].apply(lambda x: AssignMedianX(x, median_x))\n",
    "df_test['Y'] = df_test['Y'].apply(lambda x: AssignMedianY(x, median_y))\n",
    "\n",
    "## Holidays\n",
    "cal = holidaysCalendar()\n",
    "holidays = cal.holidays(start=pandas.to_datetime(df_train['Date']).min(), end=pandas.to_datetime(df_train['Date']).max())\n",
    "\n",
    "# Not comparing correctly, writing this manually\n",
    "holiday_list = []\n",
    "for day in holidays:\n",
    "    holiday_list.append(str(day).split(' ')[0])\n",
    "hol_str = str(holiday_list)\n",
    "df_train['Holiday'] = df_train['Date'].apply(lambda x: DetermineHoliday(str(x), hol_str))\n",
    "df_test['Holiday'] = df_test['Date'].apply(lambda x: DetermineHoliday(str(x), hol_str))\n",
    "\n",
    "# Format Address\n",
    "## Add address type\n",
    "df_train['LocationType'] = df_train['Address'].apply(lambda x: DetermineLocationType(x))\n",
    "df_test['LocationType'] = df_test['Address'].apply(lambda x: DetermineLocationType(x))\n",
    "\n",
    "## Form BlockID\n",
    "df_train['X'] = Normalize(df_train['X'])\n",
    "df_train['Y'] = Normalize(df_train['Y'])\n",
    "blockRow = df_train['X'].apply(lambda x: DetermineBlockRow(x))\n",
    "blockCol = df_train['Y'].apply(lambda x: DetermineBlockCol(x))\n",
    "df_train['BlockID'] = blockRow + blockCol\n",
    "\n",
    "df_test['X'] = Normalize(df_test['X'])\n",
    "df_test['Y'] = Normalize(df_test['Y'])\n",
    "blockRow = df_test['X'].apply(lambda x: DetermineBlockRow(x))\n",
    "blockCol = df_test['Y'].apply(lambda x: DetermineBlockCol(x))\n",
    "df_test['BlockID'] = blockRow + blockCol\n",
    "\n",
    "# More dates\n",
    "df_train['Month'] = df_train['Date'].apply(lambda x: x.month)\n",
    "df_train['Day'] = df_train['Date'].apply(lambda x: x.day)\n",
    "df_train['Year'] = df_train['Date'].apply(lambda x: x.year)\n",
    "df_test['Month'] = df_test['Date'].apply(lambda x: x.month)\n",
    "df_test['Day'] = df_test['Date'].apply(lambda x: x.day)\n",
    "df_test['Year'] = df_test['Date'].apply(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis\n",
    "##### Revelations:\n",
    "- Add euclidean distance using X,Y\n",
    "- Add different size 'plots' of land for X,Y (trimming)\n",
    "- Add temperature, precipitation\n",
    "- Add block or corner\n",
    "- Add time of day, day of week, weekend, week of year, season of year, holiday\n",
    "- Add early morning, morning, afternoon, early evening, evening, late evening\n",
    "- - 0500-0800, 0800-1100, 1100-1600, 1600-2100, 2100-0200, 0200-0500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Exploring data, columns, and formats\")\n",
    "print (\"\\nColumns:\")\n",
    "# View all columns\n",
    "print (df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore 'Category'\n",
    "print (\"\\nCategory:\")\n",
    "print (df_train['Category'].head(3))\n",
    "print (set(df_train['Category']))\n",
    "## OUTCOME VARIABLE\n",
    "## Possible issue with 'Trea' and 'Trespass'\n",
    "## Convert 'TREA' to 'TRESPASS'\n",
    "len(df_train[df_train['Category'] == 'TREA'])\n",
    "\n",
    "df_train['Category'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Larceny/theft the largest set of crimes. \"Other offenses\" and \"non-criminal\" are next two. This shouldn't affect how the models work, but it is interesting to note\n",
    "# Otherwise, there aren't any notable issues. \"TREA\" is probably trespassing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Descript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore 'Descript'\n",
    "print (\"\\nDescript:\")\n",
    "print (df_train['Descript'].head(3))\n",
    "## NOT IN TEST DATA\n",
    "## Does not appear to be useful. We do not train the model on this either. We will not consider it further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DayOfWeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore 'DayOfWeek'\n",
    "print (\"\\nDayOfWeek:\")\n",
    "print (df_train['DayOfWeek'].head(3))\n",
    "print (set(df_train['DayOfWeek']))\n",
    "total = 0\n",
    "for x in set(df_train['DayOfWeek']):\n",
    "    total += len(df_train[df_train['DayOfWeek'] == x])\n",
    "print (\"Missing values in DayOfWeek: \" + str(len(df_train) - total))\n",
    "## Looks good\n",
    "\n",
    "df_train['DayOfWeek'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A pretty uniform distribution, but more crimes occur on days where more people go out - like Friday and Saturday. Early in the week has less crime. This is expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PdDistrict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore 'PdDistrict'\n",
    "print (\"\\nPdDistrict:\")\n",
    "print (df_train['PdDistrict'].head(3))\n",
    "print (set(df_train['PdDistrict']))\n",
    "total = 0\n",
    "for x in set(df_train['PdDistrict']):\n",
    "    total += len(df_train[df_train['PdDistrict'] == x])\n",
    "print (\"Missing values in PdDistrict: \" + str(len(df_train) - total))\n",
    "## Looks good\n",
    "\n",
    "df_train['PdDistrict'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#While we don't know how large the police districts are or where they are located, which will play a big role in what crimes take place where, nothing about the distribution suggests that it wouldn't be useful in the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore 'Resolution'\n",
    "print (\"\\nResolution:\")\n",
    "print (df_train['Resolution'].head(3))\n",
    "## NOT IN TEST DATA\n",
    "## Does not appear to be useful. Since this does not appear in the train data and there is such a long range of strings, we decided not to focus on this and dropped it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore 'Address'\n",
    "print (\"\\nAddress:\")\n",
    "print (df_train['Address'].head(3))\n",
    "## Count Blocks, Corners\n",
    "blocks = corners = neither = 0\n",
    "for address in df_train['Address']:\n",
    "    if \"block\" in address.lower():\n",
    "        blocks += 1\n",
    "    elif \"/\" in address:\n",
    "        corners += 1\n",
    "    else:\n",
    "        neither += 1\n",
    "print (\"Number of blocks: \" + str(blocks))\n",
    "print (\"Number of corners: \" + str(corners))\n",
    "print (\"Number of neither: \" + str(neither))\n",
    "print (\"Number of both: \" + str(blocks + corners - (len(df_train) - neither)))\n",
    "\n",
    "\n",
    "## Most (~2/3) of crimes took place on a block, not on a corner. Interesting to add to model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'X' and 'Y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore 'X' and 'Y'\n",
    "print (\"\\nX and Y:\")\n",
    "print (\"Empty X coords :\" + str(len(df_train[df_train['X'] == 0])))\n",
    "print (\"Empty Y coords :\" + str(len(df_train[df_train['Y'] == 0])))\n",
    "print (\"Distinct X coords :\" + str(len(set(df_train['X']))))\n",
    "print (\"Distinct Y coords :\" + str(len(set(df_train['Y']))))\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "# Verify integrity of coordinates\n",
    "#sns.distplot(list(df_train['Y']))\n",
    "\n",
    "\n",
    "df_train = df_train[df_train['Y'] < 38]\n",
    "df_train = df_train[df_train['X'] != -120.5]\n",
    "df_test = df_test[df_test['Y'] < 38]\n",
    "df_test = df_test[df_test['X'] != -120.5]\n",
    "\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sns.distplot(list(df_train['X']))\n",
    "plt.xlabel('Distribution of Longitudes')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(list(df_train['Y']))\n",
    "plt.xlabel('Distribution of Latitudes')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a spike in crimes around the centers of the distribution which likely coincides with downtown, where there is more population density, adn we would assume a higher concentration of crimes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_train.X,df_train.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scatterplot of the crimes latitude and longitude isn't too interesting, because there are so many crimes that the density doesn't show. \n",
    "However, it does show the geographic outlines of the city.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=3, sharey=True)\n",
    "ax = axs[0]\n",
    "hb = ax.hexbin(df_train.X, df_train.Y, gridsize=50, cmap='inferno')\n",
    "ax.axis([df_train.X.min(), df_train.X.max(), df_train.Y.min(), df_train.Y.max()])\n",
    "ax.set_title(\"Hexagon binning\")\n",
    "cb = fig.colorbar(hb, ax=ax)\n",
    "cb.set_label('counts')\n",
    "\n",
    "ax = axs[1]\n",
    "hb = ax.hexbin(df_train.X, df_train.Y, gridsize=50, bins='log', cmap='inferno')\n",
    "ax.axis([df_train.X.min(), df_train.X.max(), df_train.Y.min(), df_train.Y.max()])\n",
    "ax.set_title(\"With a log color scale\")\n",
    "cb = fig.colorbar(hb, ax=ax)\n",
    "cb.set_label('log10(N)')\n",
    "\n",
    "dfXDUI= df_train.X[df_train.Category=='DRIVING UNDER THE INFLUENCE']\n",
    "dfYDUI=df_train.Y[df_train.Category=='DRIVING UNDER THE INFLUENCE']\n",
    "ax = axs[2]\n",
    "hb = ax.hexbin(dfXDUI, dfYDUI, gridsize=50, bins='log', cmap='inferno')\n",
    "ax.axis([dfXDUI.min(), dfXDUI.max(), dfYDUI.min(), dfYDUI.max()])\n",
    "ax.set_title(\"Log Scale, only DUIs\")\n",
    "cb = fig.colorbar(hb, ax=ax)\n",
    "cb.set_label('log10(N)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To get a better sense of density of crimes, we use a heatmap. \n",
    "On the left, the heatmap isn't to helpful because the scale of crimes occuring downtown is so high relative to more outlying areas.\n",
    "In the center we used a log scale, which much more clearly shows the contours of crime in the city. Interestingly, we can even see areas of high density along highways.\n",
    "To test this theory, we filtered out DUI crimes only on the right, which we would assume occur in higher prevalence along the city's major roads.\n",
    "We can clearly see this on the map. This makes it clear that even at a high level, geography is a good indicator of crime. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unemployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Mean unemployment rate for crimes=\", df_train.Value.mean())\n",
    "print (\"Median unemployment rate for crimes=\", df_train.Value.median())\n",
    "\n",
    "print (\"Mean unemployment value in CA from 1/03-5-15=\", unemployment.Value[29:178].mean())\n",
    "print (\"Median unemployment value in CA from 1/03-5-15=\", unemployment.Value[29:178].median())\n",
    "\n",
    "fig, ax= plt.subplots(nrows=1,ncols=2)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(df_train.Value[29:178])\n",
    "plt.xlabel('Months at Each Unemployment Rate')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(df_train.Value)\n",
    "plt.xlabel('Crimes Commited for Each Unemployment Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the mean unemployment rate for each crime committed was lower than the overall average unemployment rate over this period. \n",
    "This suggests that unemployment may not have a causal link to crime, at least in this setting. Note that there are a lot of other factors, including changes in population, policing strategy, among others that could affect this relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"mean daily max temp over time period =\", df_train.TMAX.mean())\n",
    "print (\"median daily max temp over time period =\", df_train.TMAX.median())\n",
    "\n",
    "print (\"mean daily max temp over each crime committed=\",df_weather.TMAX[:18241].mean())\n",
    "print (\"median daily max temp over each crime committed =\",df_weather.TMAX[:18241].median())\n",
    "\n",
    "print (\"mean daily precipitation over each crime committed\", df_train.PRCP.mean())\n",
    "print (\"median daily precipitation over each crime committed\", df_train.PRCP.median())\n",
    "\n",
    "print (\"mean daily precipitation over time period=\", df_weather.PRCP[:18241].mean())\n",
    "print (\"median daily precipitation over time period=\",df_weather.PRCP[:18241].median())\n",
    "\n",
    "print (\"ratio of crimes with precipitation to total crimes\", float(df_train.DID_RAIN.sum())/float(len(df_train.DID_RAIN)))\n",
    "print (\"ratio of days with precipitation to total days\", float(df_weather.DID_RAIN[:18241].sum())/float(len(df_weather.DID_RAIN[:18421])))\n",
    "\n",
    "fig, ax= plt.subplots(nrows=2,ncols=2)\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.hist(df_train.TMAX[:18241])\n",
    "plt.xlabel('TMAX for Each Day 01-01-2003 - 05-13-2015')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.hist(df_train.TMAX)\n",
    "plt.xlabel('Crimes Commited at Each TMAX')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.hist(df_train.PRCP[:18241])\n",
    "plt.xlabel('PRCP for Each Day 01-01-2003 - 05-13-2015')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.hist(df_train.PRCP)\n",
    "plt.xlabel('Crimes Commited at Each PRCP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be much of a difference between the mean daily max temp and precipitation and the concentration of crimes. \n",
    "However, this does not mean that there are not certain crimes that are more or less likely given the weather.\n",
    "Also, note that SF's weather is pretty steady- most days have no precipitation and mild weather. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we drop the unnecessary columns that were added during the previous steps\n",
    "# We also drop the Resolution and Descript columns which are in the train but not test data.\n",
    "# Descript is what we are predicting \n",
    "df_train = df_train.drop(['Dates', 'Address', 'Resolution', 'Descript'],axis=1)\n",
    "df_test = df_test.drop(['Dates', 'Address', 'Id'],axis=1)\n",
    "\n",
    "# Name\n",
    "train_data = df_train\n",
    "test_data = df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding\n",
    "Here we encode the data by adding dummies so that categorical data can be put into the ml models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode time\n",
    "train_data = train_data.join(pandas.get_dummies(train_data['Time'])).drop('Time', axis=1)\n",
    "test_data = test_data.join(pandas.get_dummies(test_data['Time'])).drop('Time', axis=1)\n",
    "\n",
    "# Encode LocationType\n",
    "train_data = train_data.join(pandas.get_dummies(train_data['LocationType'])).drop('LocationType', axis=1)\n",
    "test_data = test_data.join(pandas.get_dummies(test_data['LocationType'])).drop('LocationType', axis=1)\n",
    "\n",
    "# Encode PdDistrict\n",
    "train_data = train_data.join(pandas.get_dummies(train_data['PdDistrict'])).drop('PdDistrict', axis=1)\n",
    "test_data = test_data.join(pandas.get_dummies(test_data['PdDistrict'])).drop('PdDistrict', axis=1) \n",
    "\n",
    "# Encode DayOfWeek\n",
    "train_data = train_data.join(pandas.get_dummies(train_data['DayOfWeek'])).drop('DayOfWeek', axis=1)\n",
    "test_data = test_data.join(pandas.get_dummies(test_data['DayOfWeek'])).drop('DayOfWeek', axis=1)\n",
    "\n",
    "# Encode BlockID\n",
    "train_data = train_data.join(pandas.get_dummies(train_data['BlockID'])).drop('BlockID', axis=1)\n",
    "test_data = test_data.join(pandas.get_dummies(test_data['BlockID'])).drop('BlockID', axis=1)\n",
    "train_data = train_data.drop('X', axis=1)\n",
    "train_data = train_data.drop('Y', axis=1)\n",
    "test_data = test_data.drop('X', axis=1)\n",
    "test_data = test_data.drop('Y', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "le = preprocessing.LabelEncoder()\n",
    "for column in train_data.columns:\n",
    "    if train_data[column].dtype == type(object):\n",
    "        train_data[column] = le.fit_transform(train_data[column])\n",
    "for column in test_data.columns:\n",
    "    if test_data[column].dtype == type(object):\n",
    "        test_data[column] = le.fit_transform(test_data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into data/labels, train/dev\n",
    "train_labels = train_data['Category'][:-50000]\n",
    "dev_labels = train_data['Category'][-50000:]\n",
    "\n",
    "train_data = train_data[train_data.columns.difference(['Category'])][:-50000]\n",
    "dev_data = train_data[train_data.columns.difference(['Category'])][-50000:]\n",
    "\n",
    "# Labels list\n",
    "labels = list(set(train_labels))\n",
    "train_labels = train_labels.apply(lambda x: labels.index(x))\n",
    "dev_labels = dev_labels.apply(lambda x: labels.index(x))\n",
    "\n",
    "# Add missing column to test data\n",
    "test_data['89.0'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor\n",
    "Accuracy: 0.16172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "neigh = KNeighborsClassifier(n_neighbors=50)\n",
    "# Fit\n",
    "neigh.fit(train_data, train_labels) \n",
    "# Score\n",
    "print (neigh.score(dev_data, dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "result_neigh = neigh.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "SaveResults(result_neigh, \"output_neigh.csv.gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameter Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters = {'n_neighbors': range(1,2500, 250)}\n",
    "#neigh = GridSearchCV(KNeighborsClassifier(), parameters)\n",
    "#neigh.fit(train_data, train_labels)\n",
    "#print (sorted(neigh.cv_results_.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "Accuracy: 0.10008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Train\n",
    "adab = AdaBoostClassifier(DecisionTreeClassifier(max_depth=50),n_estimators=100,learning_rate=1)\n",
    "# Fit\n",
    "adab.fit(train_data, train_labels)\n",
    "# Score\n",
    "print (adab.score(dev_data, dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "result_adab = adab.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "SaveResults(result_adab, \"output_adab.csv.gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters = {'n_estimators': range(1,150, 15), 'learning_rate': range(1,3,0.5)}\n",
    "#adab_gsv = GridSearchCV(AdaBoostClassifier(), parameters)\n",
    "#adab_gsv.fit(train_data, train_labels)\n",
    "#print (sorted(adab_gsv.cv_results_.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "Accuracy: 0.13748"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Train\n",
    "rfc = RandomForestClassifier(n_estimators=120, max_depth=150, min_samples_split=20, random_state=0)\n",
    "# Fit\n",
    "rfc.fit(train_data, train_labels)\n",
    "# Score\n",
    "print (rfc.score(dev_data, dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "result_rfc = rfc.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save-2\n",
    "SaveResults(result_rfc, \"output_rfc.csv.gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters = {'n_estimators': range(1,150, 15), 'min_samples_split': range(1,25, 5), 'max_depth': range(25,100,25)}\n",
    "#rfc = GridSearchCV(RandomForestClassifier(), parameters)\n",
    "#rfc.fit(train_data, train_labels)\n",
    "#print (sorted(rfc.cv_results_.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier\n",
    "Accuracy: 0.13654"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "# Initial ensemble\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "         ('knn', neigh), ('adab', adab), ('rfc', rfc)], voting='hard', weights=[1,1,.75])\n",
    "# Fit\n",
    "voting_clf = voting_clf.fit(train_data, train_labels)\n",
    "# Score\n",
    "print (voting_clf.score(dev_data, dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "result_voting_clf = voting_clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "SaveResults(result_voting_clf, \"output_voting_clf.csv.gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural-Network\n",
    "Log-loss: 3.8651565318107606"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeatures = len(train_data.columns)\n",
    "numClasses = len(labels)\n",
    "numSamples = len(train_data)\n",
    "numTestExamples = len(dev_data)\n",
    "\n",
    "print (numFeatures)\n",
    "print (numClasses)\n",
    "print (numSamples)\n",
    "print (numTestExamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, ActivityRegularization\n",
    "from keras.layers import Flatten, Reshape\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import optimizers, regularizers\n",
    "\n",
    "# Define\n",
    "sequential_model = Sequential()\n",
    "sequential_model.add(Dense(units=numFeatures * 4, activation='hard_sigmoid', input_shape=(numFeatures,)))\n",
    "sequential_model.add(Dropout(0.2, noise_shape=None, seed=None))\n",
    "sequential_model.add(Dense(units=numFeatures * 2, kernel_regularizer=regularizers.l2(0.01)))\n",
    "sequential_model.add(Dense(units=numFeatures, activity_regularizer=regularizers.l1(0.01)))\n",
    "sequential_model.add(Dense(units=numFeatures, activation='softmax'))\n",
    "sequential_model.add(Dense(units=numClasses))\n",
    "\n",
    "# Compile\n",
    "sequential_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='Nadam',\n",
    "              metrics=['accuracy'])\n",
    "# Fit\n",
    "sequential_model.fit(train_data.values, train_labels.values, epochs=1, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "print (sequential_model.evaluate(dev_data.values, dev_labels.values, batch_size=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "result_keras = sequential_model.predict(test_data.values, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "result_keras = np.delete(result_keras, 0)\n",
    "result_keras = np.insert(result_keras, 0, header)\n",
    "\n",
    "## Convert to pandas data type and create dummies\n",
    "result = pandas.DataFrame(result_keras)\n",
    "result = pandas.get_dummies(result[0], prefix='', prefix_sep='')\n",
    "\n",
    "## Save to CSV\n",
    "result.to_csv(\"nn.csv.gzip\", compression='gzip', chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Previous ANN Model\n",
    "# (1) Parameters\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Constants\n",
    "testX = tf.constant(dev_data.values, dtype=tf.float32)\n",
    "hiddenlayer1_size = 2\n",
    "hiddenlayer2_size = 1\n",
    "miniBatchSize = 1\n",
    "\n",
    "# placeholders\n",
    "x_ = tf.placeholder(tf.float32, shape=[None, numFeatures], name='x')\n",
    "y_ = tf.placeholder(tf.int32, shape=[None], name='y')\n",
    "\n",
    "# and Variables\n",
    "w1 = tf.get_variable('w1', shape=[numFeatures, hiddenlayer1_size])\n",
    "b1 = tf.get_variable('b1', shape=[hiddenlayer1_size])\n",
    "w2 = tf.get_variable('w2', shape=[hiddenlayer1_size, numClasses])\n",
    "b2 = tf.get_variable('b2', shape=[numClasses])\n",
    "\n",
    "\n",
    "# (2) Model\n",
    "def model(input_layer):\n",
    "    hidden_layer1 = tf.nn.sigmoid(tf.matmul(input_layer, w1) + b1)\n",
    "    output_layer = tf.nn.softmax(tf.matmul(hidden_layer1, w2) + b2)\n",
    "    return output_layer\n",
    "\n",
    "# (2) Model\n",
    "def model_r(input_layer):\n",
    "    hidden_layer1 = tf.nn.relu(tf.matmul(input_layer, w1) + b1)\n",
    "    output_layer = tf.nn.softmax(tf.matmul(hidden_layer1, w2) + b2)\n",
    "    return output_layer\n",
    "    \n",
    "\n",
    "# (3) Cost\n",
    "def cost(data, labels):\n",
    "    cc = tf.sqrt(tf.square(labels - model(data)))\n",
    "    return  cc\n",
    "\n",
    "# (4) Ojbective (and solver)\n",
    "y_one_hot = tf.one_hot(y_, numClasses)\n",
    "cc = cost(x_, y_one_hot)\n",
    "gd = tf.train.GradientDescentOptimizer(0.1)\n",
    "step = gd.minimize(cc)\n",
    "test_preds = model(testX)\n",
    "test_preds_r = model_r(testX)\n",
    "output = \"\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cost_vec = []\n",
    "    cost_vec_r = []\n",
    "    for i in range(15):\n",
    "        print (i)\n",
    "        for start, end in zip(range(0, numSamples, miniBatchSize), range(miniBatchSize, numSamples, miniBatchSize)):\n",
    "            batch = train_data.values[start:end], train_labels[start:end]\n",
    "            _, cost, test__preds_r = sess.run([step, cc, test_preds_r], feed_dict={x_: batch[0], y_: batch[1]})\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
