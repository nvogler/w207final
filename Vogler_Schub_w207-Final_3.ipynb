{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project a dataset containing crime statistics in the San Francisco area from X to Y is analyzed with the goal of creating a model to predict the nature of a crime based only on its time and location. \n",
    "\n",
    "More information on the project and its dataset can be found here:\n",
    "https://www.kaggle.com/c/sf-crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats, integrate\n",
    "import seaborn as sns\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as holidaysCalendar\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "\n",
    "pandas.options.display.max_columns = 200\n",
    "pandas.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "WRITE = False\n",
    "\n",
    "# Define header (from sample submission)\n",
    "header = [\"ID\", \"ARSON\", \"ASSAULT\", \"BAD CHECKS\", \"BRIBERY\", \"BURGLARY\",\n",
    "          \"DISORDERLY CONDUCT\", \"DRIVING UNDER THE INFLUENCE\", \"DRUG/NARCOTIC\",\n",
    "          \"DRUNKENNESS\", \"EMBEZZLEMENT\", \"EXTORTION\", \"FAMILY OFFENSES\",\n",
    "          \"FORGERY/COUNTERFEITING\", \"FRAUD\", \"GAMBLING\", \"KIDNAPPING\", \"LARCENY/THEFT\",\n",
    "          \"LIQUOR LAWS\", \"LOITERING\", \"MISSING PERSON\", \"NON-CRIMINAL\", \"OTHER OFFENSES\",\n",
    "          \"PORNOGRAPHY/OBSCENE MAT\", \"PROSTITUTION\", \"RECOVERED VEHICLE\", \"ROBBERY\",\n",
    "          \"RUNAWAY\", \"SECONDARY CODES\", \"SEX OFFENSES FORCIBLE\", \"SEX OFFENSES NON FORCIBLE\",\n",
    "          \"STOLEN PROPERTY\", \"SUICIDE\", \"SUSPICIOUS OCC\", \"TREA\", \"TRESPASS\", \"VANDALISM\",\n",
    "          \"VEHICLE THEFT\", \"WARRANTS\", \"WEAPON LAWS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def PredictAndPrint(data, classifier, outputname):\n",
    "    # Predict\n",
    "    result = classifier.predict(data)\n",
    "\n",
    "    # Output\n",
    "    result = pandas.DataFrame(result)\n",
    "    result = pandas.get_dummies(result, prefix='', prefix_sep='')\n",
    "    \n",
    "    # Add null categories to make kaggle happy\n",
    "    result = result.T.reindex(header).T.fillna(0)\n",
    "    result.to_csv(outputname, compression='gzip', chunksize=1000)\n",
    "    \n",
    "def DetermineTime(hour):\n",
    "    ## Add early morning, morning, afternoon, early evening, evening, late evening\n",
    "    ## 0500-0800, 0800-1100, 1100-1600, 1600-2100, 2100-0200, 0200-0500\n",
    "    if hour <= 2:\n",
    "        return \"evening\"\n",
    "    elif hour <= 5:\n",
    "        return \"lateE\"\n",
    "    elif hour <= 8:\n",
    "        return \"earlyM\"\n",
    "    elif hour <= 11:\n",
    "        return \"morning\"\n",
    "    elif hour <= 16:\n",
    "        return \"afternoon\"\n",
    "    elif hour <= 21:\n",
    "        return \"earlyE\"\n",
    "    else:\n",
    "        return \"evening\"\n",
    "\n",
    "def DetermineLocationType(address):\n",
    "    ##Add block or corner\n",
    "    if \"block\" in address.lower():\n",
    "        return \"block\"\n",
    "    elif \"/\" in address:\n",
    "        return \"corner\"\n",
    "    return None\n",
    "\n",
    "def DetermineDayTime(time):\n",
    "    ##Add Day Time\n",
    "    time = int(time)\n",
    "    if time >= 7 and time <= 19:\n",
    "        return True\n",
    "    elif time < 7 or time >= 19:\n",
    "        return False\n",
    "    return None\n",
    "\n",
    "def Normalize(X):\n",
    "    minx = min(X)\n",
    "    maxx = max(X)\n",
    "    subx = max(X)-min(X)\n",
    "    return X.apply(lambda x: (x-minx)/subx)\n",
    "\n",
    "def DetermineBlockCol(datacol):\n",
    "    return 10 * round(datacol, 1)\n",
    " \n",
    "def DetermineBlockRow(datarow):\n",
    "    return 100 * round(datarow, 1)\n",
    "    \n",
    "def DetermineBlock(data):\n",
    "    return int(xrow) + int(yrow)\n",
    "\n",
    "def DetermineUnemp(data, unemp_dict):\n",
    "    return unemp_dict[str(data.year)][str(calendar.month_abbr[data.month])]\n",
    "\n",
    "def GenerateCoordToZipMap(filename, dict, data, read, write):\n",
    "    # Load coords\n",
    "    coords = data[['X', 'Y']]\n",
    "    \n",
    "    # Create/load dict\n",
    "    geo_dict = dict\n",
    "    if read:\n",
    "        # Load GeoDict from file\n",
    "        with open(datafile, 'r') as file:\n",
    "        geo_dict = json.load(file)\n",
    "    \n",
    "    # Generate mappings\n",
    "    for coord in coords.iterrows():\n",
    "        X = round(coord[1][1], 4)\n",
    "        Y = round(coord[1][0], 4)\n",
    "        xy = str(X) + \", \" + str(Y)\n",
    "        # Check if existing\n",
    "        if xy in geo_dict.keys():\n",
    "            print (\"Found! \" + str(geo_dict[xy]) + \" X: \" + str(X) + \"  Y: \" + str(Y))\n",
    "        else:\n",
    "            success = False\n",
    "            while not success:\n",
    "                try:\n",
    "                    location = geolocator.reverse(xy)\n",
    "                    zip = location.address.split(',')[-2].strip()\n",
    "                    geo_dict[xy] = zip\n",
    "                    print (\"Added: \" + str(zip) + \"  X: \" + str(X) + \"  Y: \" + str(Y))\n",
    "                    success = True\n",
    "                except:\n",
    "                    time.sleep(5)\n",
    "    if write:\n",
    "        # Write GeoDict to file\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write(json.dumps(geo_dict))\n",
    "    \n",
    "    return geo_dict\n",
    "\n",
    "def CoordsToZip(geo_dict, X, Y)\n",
    "    X = round(X, 4)\n",
    "    Y = round(Y, 4)\n",
    "    xy = str(X) + \", \" + str(Y)\n",
    "    if xy in geo_dict.keys():\n",
    "        return geo_dict[xy]\n",
    "    # Not found\n",
    "    return \"00000\"\n",
    "\n",
    "def DetermineHoliday(date, holidays):\n",
    "    if date in holidays:\n",
    "        return 1\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_train = pandas.read_csv(\"train.csv\")\n",
    "df_test = pandas.read_csv(\"test.csv\")\n",
    "\n",
    "# Drop duplicates\n",
    "#df_train = df_train.drop_duplicates()\n",
    "#df_test = df_test.drop_duplicates()\n",
    "\n",
    "# Shuffle\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis\n",
    "##### Revelations:\n",
    "- Add euclidean distance using X,Y\n",
    "- Add different size 'plots' of land for X,Y (trimming)\n",
    "- Add temperature, precipitation\n",
    "- Add block or corner\n",
    "- Add time of day, day of week, weekend, week of year, season of year, holiday\n",
    "- Add early morning, morning, afternoon, early evening, evening, late evening\n",
    "- - 0500-0800, 0800-1100, 1100-1600, 1600-2100, 2100-0200, 0200-0500\n",
    "- Convert 'TREA' to 'TRESPASS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring data, columns, and formats\n",
      "\n",
      "Columns:\n",
      "Index(['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict',\n",
      "       'Resolution', 'Address', 'X', 'Y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print (\"Exploring data, columns, and formats\")\n",
    "print (\"\\nColumns:\")\n",
    "# View all columns\n",
    "print (df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dates:\n",
      "0    2009-08-31 22:30:00\n",
      "1    2007-07-24 00:01:00\n",
      "2    2013-06-16 17:05:00\n",
      "Name: Dates, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Explore 'Dates'\n",
    "print (\"\\nDates:\")\n",
    "print (df_train['Dates'].head(3))\n",
    "## Add time of day, day of week, weekend, week of year, season of year, holiday\n",
    "## Add early morning, morning, afternoon, early evening, evening, late evening\n",
    "## 0500-0800, 0800-1100, 1100-1600, 1600-2100, 2100-0200, 0200-0500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category:\n",
      "0          ASSAULT\n",
      "1    LARCENY/THEFT\n",
      "2         WARRANTS\n",
      "Name: Category, dtype: object\n",
      "{'ARSON', 'SEX OFFENSES FORCIBLE', 'SEX OFFENSES NON FORCIBLE', 'TREA', 'BURGLARY', 'SUSPICIOUS OCC', 'VANDALISM', 'SUICIDE', 'DRIVING UNDER THE INFLUENCE', 'KIDNAPPING', 'EMBEZZLEMENT', 'FORGERY/COUNTERFEITING', 'EXTORTION', 'LARCENY/THEFT', 'NON-CRIMINAL', 'SECONDARY CODES', 'PROSTITUTION', 'DRUG/NARCOTIC', 'WEAPON LAWS', 'OTHER OFFENSES', 'LOITERING', 'RUNAWAY', 'STOLEN PROPERTY', 'BAD CHECKS', 'LIQUOR LAWS', 'MISSING PERSON', 'DISORDERLY CONDUCT', 'TRESPASS', 'RECOVERED VEHICLE', 'VEHICLE THEFT', 'ASSAULT', 'FRAUD', 'DRUNKENNESS', 'GAMBLING', 'ROBBERY', 'BRIBERY', 'WARRANTS', 'PORNOGRAPHY/OBSCENE MAT', 'FAMILY OFFENSES'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore 'Category'\n",
    "print (\"\\nCategory:\")\n",
    "print (df_train['Category'].head(3))\n",
    "print (set(df_train['Category']))\n",
    "## OUTCOME VARIABLE\n",
    "## Possible issue with 'Trea' and 'Trespass'\n",
    "## Convert 'TREA' to 'TRESPASS'\n",
    "len(df_train[df_train['Category'] == 'TREA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Descript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descript:\n",
      "0    AGGRAVATED ASSAULT WITH A GUN\n",
      "1     GRAND THEFT FROM LOCKED AUTO\n",
      "2                   WARRANT ARREST\n",
      "Name: Descript, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Explore 'Descript'\n",
    "print (\"\\nDescript:\")\n",
    "print (df_train['Descript'].head(3))\n",
    "## NOT IN TEST DATA\n",
    "## Does not appear to be useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DayOfWeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DayOfWeek:\n",
      "0     Monday\n",
      "1    Tuesday\n",
      "2     Sunday\n",
      "Name: DayOfWeek, dtype: object\n",
      "{'Tuesday', 'Wednesday', 'Saturday', 'Sunday', 'Monday', 'Thursday', 'Friday'}\n",
      "Missing values in DayOfWeek: 0\n"
     ]
    }
   ],
   "source": [
    "# Explore 'DayOfWeek'\n",
    "print (\"\\nDayOfWeek:\")\n",
    "print (df_train['DayOfWeek'].head(3))\n",
    "print (set(df_train['DayOfWeek']))\n",
    "total = 0\n",
    "for x in set(df_train['DayOfWeek']):\n",
    "    total += len(df_train[df_train['DayOfWeek'] == x])\n",
    "print (\"Missing values in DayOfWeek: \" + str(len(df_train) - total))\n",
    "## Looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PdDistrict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PdDistrict:\n",
      "0     BAYVIEW\n",
      "1    NORTHERN\n",
      "2        PARK\n",
      "Name: PdDistrict, dtype: object\n",
      "{'RICHMOND', 'SOUTHERN', 'NORTHERN', 'CENTRAL', 'TENDERLOIN', 'TARAVAL', 'PARK', 'INGLESIDE', 'MISSION', 'BAYVIEW'}\n",
      "Missing values in PdDistrict: 0\n"
     ]
    }
   ],
   "source": [
    "# Explore 'PdDistrict'\n",
    "print (\"\\nPdDistrict:\")\n",
    "print (df_train['PdDistrict'].head(3))\n",
    "print (set(df_train['PdDistrict']))\n",
    "total = 0\n",
    "for x in set(df_train['PdDistrict']):\n",
    "    total += len(df_train[df_train['PdDistrict'] == x])\n",
    "print (\"Missing values in PdDistrict: \" + str(len(df_train) - total))\n",
    "## Looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resolution:\n",
      "0              NONE\n",
      "1              NONE\n",
      "2    ARREST, BOOKED\n",
      "Name: Resolution, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Explore 'Resolution'\n",
    "print (\"\\nResolution:\")\n",
    "print (df_train['Resolution'].head(3))\n",
    "## NOT IN TEST DATA\n",
    "## Does not appear to be useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Address:\n",
      "0          3RD ST / EVANS AV\n",
      "1       OAK ST / FILLMORE ST\n",
      "2    700 Block of STANYAN ST\n",
      "Name: Address, dtype: object\n",
      "Number of blocks: 617231\n",
      "Number of corners: 260818\n",
      "Number of neither: 0\n",
      "Number of both: 0\n"
     ]
    }
   ],
   "source": [
    "# Explore 'Address'\n",
    "print (\"\\nAddress:\")\n",
    "print (df_train['Address'].head(3))\n",
    "## Count Blocks, Corners\n",
    "blocks = corners = neither = 0\n",
    "for address in df_train['Address']:\n",
    "    if \"block\" in address.lower():\n",
    "        blocks += 1\n",
    "    elif \"/\" in address:\n",
    "        corners += 1\n",
    "    else:\n",
    "        neither += 1\n",
    "print (\"Number of blocks: \" + str(blocks))\n",
    "print (\"Number of corners: \" + str(corners))\n",
    "print (\"Number of neither: \" + str(neither))\n",
    "print (\"Number of both: \" + str(blocks + corners - (len(df_train) - neither)))\n",
    "\n",
    "## Add block or corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'X' and 'Y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X and Y:\n",
      "Empty X coords :0\n",
      "Empty Y coords :0\n",
      "Distinct X coords :34243\n",
      "Distinct Y coords :34243\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD3CAYAAADrGWTVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhjgRURZjiBMRZTGGOBFRFmOIExFlsf8HH9IJG4bNTgIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x296b3052e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore 'X' and 'Y'\n",
    "print (\"\\nX and Y:\")\n",
    "print (\"Empty X coords :\" + str(len(df_train[df_train['X'] == 0])))\n",
    "print (\"Empty Y coords :\" + str(len(df_train[df_train['Y'] == 0])))\n",
    "print (\"Distinct X coords :\" + str(len(set(df_train['X']))))\n",
    "print (\"Distinct Y coords :\" + str(len(set(df_train['Y']))))\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "# Verify integrity of coordinates\n",
    "#sns.distplot(list(df_train['Y']))\n",
    "sns.distplot(list(df_train['X']))\n",
    "\n",
    "df_train = df_train[df_train['Y'] < 38]\n",
    "df_train = df_train[df_train['X'] != -120.5]\n",
    "df_test = df_test[df_test['Y'] < 38]\n",
    "df_test = df_test[df_test['X'] != -120.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "- Add euclidean distance using X,Y\n",
    "- Add different size 'plots' of land for X,Y using 10x10 grid\n",
    "- Add temperature, precipitation\n",
    "- Add block or corner\n",
    "- Add time of day, week of year, season of year, holiday, sun up or down\n",
    "- Add early morning, morning, afternoon, early evening, evening, late evening\n",
    "- - 0500-0800, 0800-1100, 1100-1600, 1600-2100, 2100-0200, 0200-0500\n",
    "- Convert 'TREA' to 'TRESPASS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize with dummy variables\n",
    "#dummies_df_train = df_train[['DayOfWeek', 'PdDistrict']]\n",
    "#dummies_df_train = pandas.get_dummies(dummies_df_train)\n",
    "#dummies_df_test = df_test[['DayOfWeek', 'PdDistrict']]\n",
    "#dummies_df_test = pandas.get_dummies(dummies_df_test)\n",
    "\n",
    "# Format Dates\n",
    "## Hour of day\n",
    "df_train['Hour'] = pandas.to_datetime(df_train['Dates']).dt.hour\n",
    "df_test['Hour'] = pandas.to_datetime(df_test['Dates']).dt.hour\n",
    "\n",
    "## Time of day\n",
    "df_train['Time'] = df_train['Hour'].apply(lambda x: DetermineTime(x))\n",
    "df_test['Time'] = df_test['Hour'].apply(lambda x: DetermineTime(x))\n",
    "\n",
    "## Day\n",
    "df_train['DayTime'] = df_train['Hour'].apply(lambda x: DetermineDayTime(x))\n",
    "df_test['DayTime'] = df_test['Hour'].apply(lambda x: DetermineDayTime(x))\n",
    "\n",
    "## Week of year\n",
    "df_train['Week'] = pandas.to_datetime(df_train['Dates']).dt.week\n",
    "df_test['Week'] = pandas.to_datetime(df_test['Dates']).dt.week\n",
    "\n",
    "## Season of year\n",
    "#df_train['Season'] = pandas.to_datetime(df_train['Dates']).dt.week\n",
    "#df_test['Season'] = pandas.to_datetime(df_test['Dates']).dt.week\n",
    "\n",
    "## Reduce to date\n",
    "df_train['Date'] = pandas.to_datetime(df_train['Dates']).dt.date\n",
    "df_test['Date'] = pandas.to_datetime(df_test['Dates']).dt.date\n",
    "\n",
    "# Adjust out of bounds values\n",
    "df_train[df_train['Y'] < 38] = np.median(df_train['Y'])\n",
    "df_train[df_train['X'] != -120.5] = np.median(df_train['X'])\n",
    "df_test[df_test['Y'] < 38] = np.median(df_test['Y'])\n",
    "df_test[df_test['X'] != -120.5] = np.median(df_test['X'])\n",
    "\n",
    "## Holidays\n",
    "cal = holidaysCalendar()\n",
    "holidays = cal.holidays(start=pandas.to_datetime(df_train['Date']).min(), end=pandas.to_datetime(df_train['Date']).max())\n",
    "\n",
    "# Not comparing correctly, writing this manually\n",
    "holiday_list = []\n",
    "for day in holidays:\n",
    "    holiday_list.append(str(day).split(' ')[0])\n",
    "hol_str = str(holiday_list)\n",
    "df_train['Holiday'] = df_train['Date'].apply(lambda x: DetermineHoliday(str(x), hol_str))\n",
    "\n",
    "# Format Address\n",
    "## Add address type\n",
    "df_train['LocationType'] = df_train['Address'].apply(lambda x: DetermineLocationType(x))\n",
    "df_test['LocationType'] = df_test['Address'].apply(lambda x: DetermineLocationType(x))\n",
    "\n",
    "## Form BlockID\n",
    "df_train['X'] = Normalize(df_train['X'])\n",
    "df_train['Y'] = Normalize(df_train['Y'])\n",
    "blockRow = df_train['X'].apply(lambda x: DetermineBlockRow(x))\n",
    "blockCol = df_train['Y'].apply(lambda x: DetermineBlockCol(x))\n",
    "df_train['BlockID'] = blockRow + blockCol\n",
    "df_train = df_train[df_train['BlockID'] != 89]\n",
    "\n",
    "df_test['X'] = Normalize(df_test['X'])\n",
    "df_test['Y'] = Normalize(df_test['Y'])\n",
    "blockRow = df_test['X'].apply(lambda x: DetermineBlockRow(x))\n",
    "blockCol = df_test['Y'].apply(lambda x: DetermineBlockCol(x))\n",
    "df_test['BlockID'] = blockRow + blockCol\n",
    "# Clean\n",
    "del blockRow, blockCol\n",
    "\n",
    "# Add Temperature\n",
    "\n",
    "# More dates\n",
    "df_train['Month'] = df_train['Date'].apply(lambda x: x.month)\n",
    "df_train['Day'] = df_train['Date'].apply(lambda x: x.day)\n",
    "df_train['Year'] = df_train['Date'].apply(lambda x: x.year)\n",
    "\n",
    "# Add Unemployment\n",
    "## Unemployment data\n",
    "## Load from csv\n",
    "unemp = pandas.read_csv(\"cal_unemployment.csv\")\n",
    "unemp = unemp[['Label', 'Value']]\n",
    "## Load into dict\n",
    "unemp_dict = {}\n",
    "for i in range(len(unemp)):\n",
    "    year = unemp['Label'][i].split(' ')\n",
    "    month = year[1]\n",
    "    year = year[0]\n",
    "    if year in unemp_dict.keys():\n",
    "        unemp_dict[year][month] = unemp['Value'][i]\n",
    "    else:\n",
    "        unemp_dict[year] = {}\n",
    "        unemp_dict[year][month] = unemp['Value'][i]\n",
    "## Add column\n",
    "df_train['Unemp'] = df_train['Date'].apply(lambda x: DetermineUnemp(x, unemp_dct))\n",
    "df_train['Unemp'] = Normalize(df_train['Unemp'])\n",
    "\n",
    "# Add Zip code\n",
    "geolocator = Nominatim()\n",
    "geo_dict = {}\n",
    "coords = df_train[['X', 'Y']]\n",
    "    \n",
    "df_train['Zip'] = df_train[['X','Y']].apply(lambda row: CoordsToZip(geo_dict, df_train['X'], df_train['Y']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df_train = df_train.drop(['Date', 'Dates', 'Address', 'Resolution', 'Descript'],axis=1)\n",
    "df_test = df_test.drop(['Date', 'Dates', 'Address', 'Id'],axis=1)\n",
    "\n",
    "# Name\n",
    "train_data = df_train\n",
    "test_data = df_test\n",
    "\n",
    "# Clean\n",
    "del df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode time\n",
    "train_data = train_data.join(pandas.get_dummies(train_data['Time'])).drop('Time', axis=1)\n",
    "test_data = test_data.join(pandas.get_dummies(test_data['Time'])).drop('Time', axis=1)\n",
    "\n",
    "# Encode LocationType\n",
    "train_data = train_data.join(pandas.get_dummies(train_data['LocationType'])).drop('LocationType', axis=1)\n",
    "test_data = test_data.join(pandas.get_dummies(test_data['LocationType'])).drop('LocationType', axis=1)\n",
    "\n",
    "# Encode PdDistrict\n",
    "train_data = train_data.join(pandas.get_dummies(train_data['PdDistrict'])).drop('PdDistrict', axis=1)\n",
    "test_data = test_data.join(pandas.get_dummies(test_data['PdDistrict'])).drop('PdDistrict', axis=1) \n",
    "\n",
    "# Encode DayOfWeek\n",
    "train_data = train_data.join(pandas.get_dummies(train_data['DayOfWeek'])).drop('DayOfWeek', axis=1)\n",
    "test_data = test_data.join(pandas.get_dummies(test_data['DayOfWeek'])).drop('DayOfWeek', axis=1)\n",
    "\n",
    "# Encode BlockID\n",
    "train_data = train_data.join(pandas.get_dummies(train_data['BlockID'])).drop('BlockID', axis=1)\n",
    "test_data = test_data.join(pandas.get_dummies(test_data['BlockID'])).drop('BlockID', axis=1)\n",
    "train_data = train_data.drop('X', axis=1)\n",
    "train_data = train_data.drop('Y', axis=1)\n",
    "test_data = test_data.drop('X', axis=1)\n",
    "test_data = test_data.drop('Y', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "le = preprocessing.LabelEncoder()\n",
    "for column in train_data.columns:\n",
    "    if train_data[column].dtype == type(object):\n",
    "        train_data[column] = le.fit_transform(train_data[column])\n",
    "for column in test_data.columns:\n",
    "    if test_data[column].dtype == type(object):\n",
    "        test_data[column] = le.fit_transform(test_data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into data/labels, train/dev\n",
    "train_labels = train_data['Category'][:-50000]\n",
    "dev_labels = train_data['Category'][-50000:]\n",
    "\n",
    "train_data = train_data[train_data.columns.difference(['Category'])][:-50000]\n",
    "dev_data = train_data[train_data.columns.difference(['Category'])][-50000:]\n",
    "\n",
    "\n",
    "# Labels list\n",
    "labels = list(set(train_labels))\n",
    "train_labels = train_labels.apply(lambda x: labels.index(x))\n",
    "dev_labels = dev_labels.apply(lambda x: labels.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "##### K-Nearest Neighbor with k=50\n",
    "\n",
    "Need to add GridSearchCV here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#parameters = {'n_neighbors': range(5,50, 5)}\n",
    "\n",
    "#neigh = GridSearchCV(KNeighborsClassifier(), parameters)\n",
    "#neigh.fit(train_data, train_labels)\n",
    "#print (sorted(neigh.cv_results_.keys()))\n",
    "\n",
    "# Train KNN Classifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=500)\n",
    "\n",
    "# Fit\n",
    "neigh.fit(train_data, train_labels) \n",
    "\n",
    "# Score\n",
    "print (neigh.score(dev_data, dev_labels))\n",
    "\n",
    "#if WRITE:\n",
    "#    PredictAndPrint(test_data, neigh, \"output_knn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "##### Final Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(color_codes=True)\n",
    "# Verify integrity of coordinates\n",
    "#sns.distplot(list(df_train['Y']))\n",
    "sns.distplot(list(test_data['X']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "adab = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5),n_estimators=100,\n",
    "    learning_rate=1)\n",
    "\n",
    "adab.fit(train_data, train_labels)\n",
    "\n",
    "adab.score(dev_data, dev_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "##### Final Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm_clf = svm.SVC(decision_function_shape='ovo')\n",
    "\n",
    "svm_clf.fit(train_data, train_labels)\n",
    "\n",
    "svm_clf.score(dev_data, dev_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "##### Final Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#parameters = {'n_estimators': range(10,50, 10), 'min_samples_split': range(2,5, 1)}\n",
    "#rfc = GridSearchCV(RandomForestClassifier(), parameters)\n",
    "#rfc.fit(train_data, train_labels)\n",
    "#print (sorted(rfc.cv_results_.keys()))\n",
    "\n",
    "# Train RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=11, max_depth=None, min_samples_split=10, random_state=0)\n",
    "\n",
    "# Fit\n",
    "rfc.fit(train_data, train_labels)\n",
    "\n",
    "# Score\n",
    "print (rfc.score(dev_data, dev_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural-Network\n",
    "##### Final Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "39\n",
      "827826\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "numFeatures = len(train_data.columns)\n",
    "numClasses = len(labels)\n",
    "numSamples = len(train_data)\n",
    "numTestExamples = len(dev_data)\n",
    "\n",
    "print (numFeatures)\n",
    "print (numClasses)\n",
    "print (numSamples)\n",
    "print (numTestExamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "827826/827826 [==============================] - 82s 99us/step - loss: 2.5402 - acc: 0.2348\n",
      "Epoch 2/20\n",
      "827826/827826 [==============================] - 82s 99us/step - loss: 2.5086 - acc: 0.2397\n",
      "Epoch 3/20\n",
      "827826/827826 [==============================] - 80s 97us/step - loss: 2.5008 - acc: 0.2413\n",
      "Epoch 4/20\n",
      "827826/827826 [==============================] - 80s 97us/step - loss: 2.4947 - acc: 0.2428\n",
      "Epoch 5/20\n",
      "827826/827826 [==============================] - 79s 96us/step - loss: 2.4905 - acc: 0.2440\n",
      "Epoch 6/20\n",
      "827826/827826 [==============================] - 80s 97us/step - loss: 2.4871 - acc: 0.2440\n",
      "Epoch 7/20\n",
      "827826/827826 [==============================] - 91s 110us/step - loss: 2.4842 - acc: 0.2449\n",
      "Epoch 8/20\n",
      "827826/827826 [==============================] - 89s 107us/step - loss: 2.4821 - acc: 0.2457\n",
      "Epoch 9/20\n",
      "827826/827826 [==============================] - 90s 109us/step - loss: 2.4799 - acc: 0.2457\n",
      "Epoch 10/20\n",
      "827826/827826 [==============================] - 85s 103us/step - loss: 2.4782 - acc: 0.2463\n",
      "Epoch 11/20\n",
      "827826/827826 [==============================] - 77s 93us/step - loss: 2.4765 - acc: 0.2465\n",
      "Epoch 12/20\n",
      "827826/827826 [==============================] - 78s 95us/step - loss: 2.4751 - acc: 0.2467\n",
      "Epoch 13/20\n",
      "827826/827826 [==============================] - 81s 98us/step - loss: 2.4739 - acc: 0.2472\n",
      "Epoch 14/20\n",
      "827826/827826 [==============================] - 81s 98us/step - loss: 2.4728 - acc: 0.2476\n",
      "Epoch 15/20\n",
      "827826/827826 [==============================] - 81s 97us/step - loss: 2.4717 - acc: 0.2477\n",
      "Epoch 16/20\n",
      "827826/827826 [==============================] - 87s 105us/step - loss: 2.4706 - acc: 0.2481\n",
      "Epoch 17/20\n",
      "827826/827826 [==============================] - 87s 105us/step - loss: 2.4697 - acc: 0.2479\n",
      "Epoch 18/20\n",
      "827826/827826 [==============================] - 87s 105us/step - loss: 2.4689 - acc: 0.2483\n",
      "Epoch 19/20\n",
      "827826/827826 [==============================] - 87s 105us/step - loss: 2.4682 - acc: 0.2485\n",
      "Epoch 20/20\n",
      "827826/827826 [==============================] - 86s 104us/step - loss: 2.4674 - acc: 0.2487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x296a2b357b8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, ActivityRegularization\n",
    "from keras.layers import Flatten, Reshape\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import optimizers\n",
    "\n",
    "# Define\n",
    "sequential_model = Sequential()\n",
    "sequential_model.add(Dense(units=numFeatures * 2, activation='hard_sigmoid', input_shape=(numFeatures,)))\n",
    "sequential_model.add(Dense(units=int(numFeatures), activation='softmax' ))\n",
    "#sequential_model.add(Dropout(0.05, noise_shape=None, seed=None))\n",
    "\n",
    "# Compile\n",
    "sequential_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='Adamax',\n",
    "              metrics=['accuracy'])\n",
    "# Fit\n",
    "sequential_model.fit(train_data.values, train_labels.values, epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "827826/827826 [==============================] - 90s 108us/step - loss: 2.5260 - acc: 0.2374\n",
      "Epoch 2/20\n",
      "827826/827826 [==============================] - 90s 108us/step - loss: 2.4954 - acc: 0.2429\n",
      "Epoch 3/20\n",
      "827826/827826 [==============================] - 86s 104us/step - loss: 2.4873 - acc: 0.2443\n",
      "Epoch 4/20\n",
      "827826/827826 [==============================] - 93s 112us/step - loss: 2.4825 - acc: 0.2456\n",
      "Epoch 5/20\n",
      "827826/827826 [==============================] - 92s 111us/step - loss: 2.4795 - acc: 0.2461\n",
      "Epoch 6/20\n",
      "827826/827826 [==============================] - 90s 109us/step - loss: 2.4768 - acc: 0.2469\n",
      "Epoch 7/20\n",
      "827826/827826 [==============================] - 90s 109us/step - loss: 2.4747 - acc: 0.2471\n",
      "Epoch 8/20\n",
      "827826/827826 [==============================] - 90s 108us/step - loss: 2.4729 - acc: 0.2476\n",
      "Epoch 9/20\n",
      "827826/827826 [==============================] - 91s 110us/step - loss: 2.4712 - acc: 0.2482\n",
      "Epoch 10/20\n",
      "827826/827826 [==============================] - 89s 107us/step - loss: 2.4698 - acc: 0.2478\n",
      "Epoch 11/20\n",
      "827826/827826 [==============================] - 89s 107us/step - loss: 2.4686 - acc: 0.2482\n",
      "Epoch 12/20\n",
      "827826/827826 [==============================] - 91s 109us/step - loss: 2.4676 - acc: 0.2486\n",
      "Epoch 13/20\n",
      "827826/827826 [==============================] - 89s 108us/step - loss: 2.4665 - acc: 0.2485\n",
      "Epoch 14/20\n",
      "827826/827826 [==============================] - 87s 105us/step - loss: 2.4656 - acc: 0.2491\n",
      "Epoch 15/20\n",
      "827826/827826 [==============================] - 89s 107us/step - loss: 2.4649 - acc: 0.2489\n",
      "Epoch 16/20\n",
      "827826/827826 [==============================] - 85s 102us/step - loss: 2.4641 - acc: 0.2490\n",
      "Epoch 17/20\n",
      "827826/827826 [==============================] - 88s 107us/step - loss: 2.4636 - acc: 0.2494\n",
      "Epoch 18/20\n",
      "827826/827826 [==============================] - 89s 108us/step - loss: 2.4631 - acc: 0.2492\n",
      "Epoch 19/20\n",
      "827826/827826 [==============================] - 86s 104us/step - loss: 2.4624 - acc: 0.2496\n",
      "Epoch 20/20\n",
      "827826/827826 [==============================] - 90s 109us/step - loss: 2.4622 - acc: 0.2498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x296ac4f30f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define\n",
    "sequential_model = Sequential()\n",
    "sequential_model.add(Dense(units=numFeatures * 2, activation='hard_sigmoid', input_shape=(numFeatures,)))\n",
    "sequential_model.add(Dense(units=int(numFeatures), activation='softmax' ))\n",
    "#sequential_model.add(Dropout(0.05, noise_shape=None, seed=None))\n",
    "\n",
    "# Compile\n",
    "sequential_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='Nadam',\n",
    "              metrics=['accuracy'])\n",
    "# Fit\n",
    "sequential_model.fit(train_data.values, train_labels.values, epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define\n",
    "sequential_model = Sequential()\n",
    "sequential_model.add(Dense(units=numFeatures * 2, activation='tanh', input_shape=(numFeatures,)))\n",
    "sequential_model.add(Dense(units=int(numFeatures), activation='softmax' ))\n",
    "#sequential_model.add(Dropout(0.05, noise_shape=None, seed=None))\n",
    "\n",
    "# Compile\n",
    "sequential_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='Nadam',\n",
    "              metrics=['accuracy'])\n",
    "# Fit\n",
    "sequential_model.fit(train_data.values, train_labels.values, epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "827826/827826 [==============================] - 118s 143us/step - loss: 2.5226 - acc: 0.2375\n",
      "Epoch 2/20\n",
      "827826/827826 [==============================] - 113s 137us/step - loss: 2.4942 - acc: 0.2430\n",
      "Epoch 3/20\n",
      "827826/827826 [==============================] - 102s 123us/step - loss: 2.4877 - acc: 0.2442\n",
      "Epoch 4/20\n",
      "827826/827826 [==============================] - 105s 127us/step - loss: 2.4841 - acc: 0.2453\n",
      "Epoch 5/20\n",
      "827826/827826 [==============================] - 116s 140us/step - loss: 2.4815 - acc: 0.2457\n",
      "Epoch 6/20\n",
      "827826/827826 [==============================] - 117s 141us/step - loss: 2.4802 - acc: 0.2460\n",
      "Epoch 7/20\n",
      "827826/827826 [==============================] - 116s 141us/step - loss: 2.4790 - acc: 0.2466\n",
      "Epoch 8/20\n",
      "827826/827826 [==============================] - 116s 140us/step - loss: 2.4781 - acc: 0.2462\n",
      "Epoch 9/20\n",
      "827826/827826 [==============================] - 113s 136us/step - loss: 2.4773 - acc: 0.2468\n",
      "Epoch 10/20\n",
      "827826/827826 [==============================] - 89s 108us/step - loss: 2.4766 - acc: 0.2471\n",
      "Epoch 11/20\n",
      "827826/827826 [==============================] - 89s 107us/step - loss: 2.4763 - acc: 0.2469\n",
      "Epoch 12/20\n",
      "827826/827826 [==============================] - 90s 109us/step - loss: 2.4757 - acc: 0.2470\n",
      "Epoch 13/20\n",
      "827826/827826 [==============================] - 90s 108us/step - loss: 2.4753 - acc: 0.2472\n",
      "Epoch 14/20\n",
      "827826/827826 [==============================] - 88s 107us/step - loss: 2.4750 - acc: 0.2474\n",
      "Epoch 15/20\n",
      "827826/827826 [==============================] - 90s 108us/step - loss: 2.4748 - acc: 0.2476\n",
      "Epoch 16/20\n",
      "827826/827826 [==============================] - 90s 108us/step - loss: 2.4748 - acc: 0.2471\n",
      "Epoch 17/20\n",
      "827826/827826 [==============================] - 89s 107us/step - loss: 2.4744 - acc: 0.2478\n",
      "Epoch 18/20\n",
      "827826/827826 [==============================] - 95s 114us/step - loss: 2.4743 - acc: 0.2475\n",
      "Epoch 19/20\n",
      "827826/827826 [==============================] - 79401s 96ms/step - loss: 2.4741 - acc: 0.2475\n",
      "Epoch 20/20\n",
      "827826/827826 [==============================] - 107s 130us/step - loss: 2.4739 - acc: 0.2475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x296a8712d30>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define\n",
    "sequential_model = Sequential()\n",
    "sequential_model.add(Dense(units=numFeatures * 2, activation='hard_sigmoid', input_shape=(numFeatures,)))\n",
    "sequential_model.add(Dense(units=int(numFeatures), activation='softmax' ))\n",
    "#sequential_model.add(Dropout(0.05, noise_shape=None, seed=None))\n",
    "\n",
    "# Compile\n",
    "sequential_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='Nadam',\n",
    "              metrics=['accuracy'])\n",
    "# Fit\n",
    "sequential_model.fit(train_data.values, train_labels.values, epochs=20, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "#print (sequential_model.evaluate(dev_data.values, dev_labels.values, batch_size=100))\n",
    "\n",
    "# Predict\n",
    "keras_seq_result = sequential_model.predict(test_data.values, batch_size=100)\n",
    "#print (keras_seq_result)\n",
    "# Output\n",
    "keras_seq_result = pandas.DataFrame(keras_seq_result)\n",
    "print (keras_seq_resul.head())\n",
    "keras_seq_result = pandas.get_dummies(keras_seq_result, prefix='', prefix_sep='')\n",
    "print (keras_seq_result.head())\n",
    "# Add null categories to make kaggle happy\n",
    "keras_seq_result = keras_seq_result.T.reindex(header).T.fillna(0)\n",
    "keras_seq_result.to_csv(\"output_keras_seq.csv\", compression='gzip', chunksize=1000)\n",
    "       \n",
    "print (max(keras_seq_result[0]))\n",
    "print (np.argwhere(keras_seq_result[0] == max(keras_seq_result[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ## 2.45\n",
    "sequential_model = Sequential()\n",
    "sequential_model.add(Dense(units=numFeatures * 2, activation='hard_sigmoid', input_shape=(numFeatures,)))\n",
    "sequential_model.add(Dense(units=int(numFeatures), activation='softmax' ))\n",
    "#sequential_model.add(Dropout(0.05, noise_shape=None, seed=None))\n",
    "\n",
    "# Compile\n",
    "sequential_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='Nadam',\n",
    "              metrics=['accuracy'])\n",
    "# Fit\n",
    "sequential_model.fit(train_data.values, train_labels.values, epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Parameters\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Constants\n",
    "testX = tf.constant(dev_data.values, dtype=tf.float32)\n",
    "hiddenlayer1_size = 2\n",
    "hiddenlayer2_size = 1\n",
    "miniBatchSize = 1\n",
    "\n",
    "# placeholders\n",
    "x_ = tf.placeholder(tf.float32, shape=[None, numFeatures], name='x')\n",
    "y_ = tf.placeholder(tf.int32, shape=[None], name='y')\n",
    "\n",
    "# and Variables\n",
    "w1 = tf.get_variable('w1', shape=[numFeatures, hiddenlayer1_size])\n",
    "b1 = tf.get_variable('b1', shape=[hiddenlayer1_size])\n",
    "w2 = tf.get_variable('w2', shape=[hiddenlayer1_size, numClasses])\n",
    "b2 = tf.get_variable('b2', shape=[numClasses])\n",
    "\n",
    "\n",
    "# (2) Model\n",
    "def model(input_layer):\n",
    "    hidden_layer1 = tf.nn.sigmoid(tf.matmul(input_layer, w1) + b1)\n",
    "    output_layer = tf.nn.softmax(tf.matmul(hidden_layer1, w2) + b2)\n",
    "    return output_layer\n",
    "\n",
    "# (2) Model\n",
    "def model_r(input_layer):\n",
    "    hidden_layer1 = tf.nn.relu(tf.matmul(input_layer, w1) + b1)\n",
    "    output_layer = tf.nn.softmax(tf.matmul(hidden_layer1, w2) + b2)\n",
    "    return output_layer\n",
    "    \n",
    "\n",
    "# (3) Cost\n",
    "def cost(data, labels):\n",
    "    cc = tf.sqrt(tf.square(labels - model(data)))\n",
    "    return  cc\n",
    "\n",
    "# (4) Ojbective (and solver)\n",
    "y_one_hot = tf.one_hot(y_, numClasses)\n",
    "cc = cost(x_, y_one_hot)\n",
    "gd = tf.train.GradientDescentOptimizer(0.1)\n",
    "step = gd.minimize(cc)\n",
    "test_preds = model(testX)\n",
    "test_preds_r = model_r(testX)\n",
    "output = \"\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cost_vec = []\n",
    "    cost_vec_r = []\n",
    "    for i in range(15):\n",
    "        print (i)\n",
    "        for start, end in zip(range(0, numSamples, miniBatchSize), range(miniBatchSize, numSamples, miniBatchSize)):\n",
    "            batch = train_data.values[start:end], train_labels[start:end]\n",
    "            _, cost, test__preds_r = sess.run([step, cc, test_preds_r], feed_dict={x_: batch[0], y_: batch[1]})\n",
    "    \n",
    "    prediction=tf.argmax(test_preds_r,axis=1)\n",
    "    output = prediction.eval(feed_dict={x_: test_data.values})\n",
    "    print (\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "# Initial ensemble\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "         ('knn', knn), ('adab', adab), ('keras_nn', smnn)], voting='hard', weights=[.75,.75,1])\n",
    "# Fit\n",
    "voting_clf = voting_clf.fit(train_data.values, train_labels.values)\n",
    "# Predict, print\n",
    "PredictAndPrint(test_data, voting_clf, \"voting_clf_1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
