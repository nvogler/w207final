{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project a dataset containing crime statistics in the San Francisco area from X to Y is analyzed with the goal of creating a model to predict the nature of a crime based only on its time and location. \n",
    "\n",
    "More information on the project and its dataset can be found here:\n",
    "https://www.kaggle.com/c/sf-crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as holidaysCalendar\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "\n",
    "pandas.options.display.max_columns = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "WRITE = False\n",
    "\n",
    "# Define header (from sample submission)\n",
    "header = [\"ID\", \"ARSON\", \"ASSAULT\", \"BAD CHECKS\", \"BRIBERY\", \"BURGLARY\",\n",
    "          \"DISORDERLY CONDUCT\", \"DRIVING UNDER THE INFLUENCE\", \"DRUG/NARCOTIC\",\n",
    "          \"DRUNKENNESS\", \"EMBEZZLEMENT\", \"EXTORTION\", \"FAMILY OFFENSES\",\n",
    "          \"FORGERY/COUNTERFEITING\", \"FRAUD\", \"GAMBLING\", \"KIDNAPPING\", \"LARCENY/THEFT\",\n",
    "          \"LIQUOR LAWS\", \"LOITERING\", \"MISSING PERSON\", \"NON-CRIMINAL\", \"OTHER OFFENSES\",\n",
    "          \"PORNOGRAPHY/OBSCENE MAT\", \"PROSTITUTION\", \"RECOVERED VEHICLE\", \"ROBBERY\",\n",
    "          \"RUNAWAY\", \"SECONDARY CODES\", \"SEX OFFENSES FORCIBLE\", \"SEX OFFENSES NON FORCIBLE\",\n",
    "          \"STOLEN PROPERTY\", \"SUICIDE\", \"SUSPICIOUS OCC\", \"TREA\", \"TRESPASS\", \"VANDALISM\",\n",
    "          \"VEHICLE THEFT\", \"WARRANTS\", \"WEAPON LAWS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def PredictAndPrint(data, classifier, outputname):\n",
    "    # Predict\n",
    "    result = classifier.predict(data)\n",
    "\n",
    "    # Output\n",
    "    result = pandas.DataFrame(result)\n",
    "    result = pandas.get_dummies(result, prefix='', prefix_sep='')\n",
    "    \n",
    "    # Add null categories to make kaggle happy\n",
    "    result = result.T.reindex(header).T.fillna(0)\n",
    "    result.to_csv(outputname, compression='gzip', chunksize=1000)\n",
    "    \n",
    "def DetermineTime(hour):\n",
    "    ## Add early morning, morning, afternoon, early evening, evening, late evening\n",
    "    ## 0500-0800, 0800-1100, 1100-1600, 1600-2100, 2100-0200, 0200-0500\n",
    "    if hour <= 2:\n",
    "        return \"evening\"\n",
    "    elif hour <= 5:\n",
    "        return \"lateE\"\n",
    "    elif hour <= 8:\n",
    "        return \"earlyM\"\n",
    "    elif hour <= 11:\n",
    "        return \"morning\"\n",
    "    elif hour <= 16:\n",
    "        return \"afternoon\"\n",
    "    elif hour <= 21:\n",
    "        return \"earlyE\"\n",
    "    else:\n",
    "        return \"evening\"\n",
    "\n",
    "def DetermineLocationType(address):\n",
    "    ##Add block or corner\n",
    "    if \"block\" in address.lower():\n",
    "        return \"block\"\n",
    "    elif \"/\" in address:\n",
    "        return \"corner\"\n",
    "    return None\n",
    "\n",
    "def DetermineDayTime(time):\n",
    "    ##Add Day Time\n",
    "    time = int(time)\n",
    "    if time >= 7 and time <= 19:\n",
    "        return True\n",
    "    elif time < 7 or time >= 19:\n",
    "        return False\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_train = pandas.read_csv(\"train.csv\")\n",
    "df_test = pandas.read_csv(\"test.csv\")\n",
    "\n",
    "# Drop duplicates\n",
    "df_train = df_train.drop_duplicates()\n",
    "df_test = df_test.drop_duplicates()\n",
    "\n",
    "# Shuffle\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis\n",
    "##### Revelations:\n",
    "- Add euclidean distance using X,Y\n",
    "- Add different size 'plots' of land for X,Y (trimming)\n",
    "- Add temperature, precipitation\n",
    "- Add block or corner\n",
    "- Add time of day, day of week, weekend, week of year, season of year, holiday\n",
    "- Add early morning, morning, afternoon, early evening, evening, late evening\n",
    "- - 0500-0800, 0800-1100, 1100-1600, 1600-2100, 2100-0200, 0200-0500\n",
    "- Convert 'TREA' to 'TRESPASS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring data, columns, and formats\n",
      "\n",
      "Columns:\n",
      "Index(['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict',\n",
      "       'Resolution', 'Address', 'X', 'Y'],\n",
      "      dtype='object')\n",
      "\n",
      "Dates:\n",
      "0    2011-01-02 14:00:00\n",
      "1    2009-11-12 15:50:00\n",
      "2    2007-03-24 23:54:00\n",
      "Name: Dates, dtype: object\n",
      "\n",
      "Category:\n",
      "0             LARCENY/THEFT\n",
      "1            OTHER OFFENSES\n",
      "2    FORGERY/COUNTERFEITING\n",
      "Name: Category, dtype: object\n",
      "{'TRESPASS', 'SEX OFFENSES NON FORCIBLE', 'EMBEZZLEMENT', 'FORGERY/COUNTERFEITING', 'FAMILY OFFENSES', 'DISORDERLY CONDUCT', 'PROSTITUTION', 'VEHICLE THEFT', 'OTHER OFFENSES', 'STOLEN PROPERTY', 'KIDNAPPING', 'LARCENY/THEFT', 'BURGLARY', 'EXTORTION', 'ASSAULT', 'NON-CRIMINAL', 'DRUNKENNESS', 'GAMBLING', 'ARSON', 'RUNAWAY', 'MISSING PERSON', 'SECONDARY CODES', 'LOITERING', 'RECOVERED VEHICLE', 'LIQUOR LAWS', 'FRAUD', 'WARRANTS', 'ROBBERY', 'SEX OFFENSES FORCIBLE', 'BAD CHECKS', 'BRIBERY', 'DRUG/NARCOTIC', 'PORNOGRAPHY/OBSCENE MAT', 'SUICIDE', 'DRIVING UNDER THE INFLUENCE', 'TREA', 'SUSPICIOUS OCC', 'WEAPON LAWS', 'VANDALISM'}\n",
      "\n",
      "Descript:\n",
      "0    GRAND THEFT FROM LOCKED AUTO\n",
      "1             PROBATION VIOLATION\n",
      "2     MONEY, CHANGING FACE AMOUNT\n",
      "Name: Descript, dtype: object\n",
      "\n",
      "DayOfWeek:\n",
      "0      Sunday\n",
      "1    Thursday\n",
      "2    Saturday\n",
      "Name: DayOfWeek, dtype: object\n",
      "{'Monday', 'Saturday', 'Tuesday', 'Sunday', 'Wednesday', 'Friday', 'Thursday'}\n",
      "Missing values in DayOfWeek: 0\n",
      "\n",
      "PdDistrict:\n",
      "0       MISSION\n",
      "1    TENDERLOIN\n",
      "2      SOUTHERN\n",
      "Name: PdDistrict, dtype: object\n",
      "{'TARAVAL', 'TENDERLOIN', 'INGLESIDE', 'PARK', 'NORTHERN', 'BAYVIEW', 'MISSION', 'SOUTHERN', 'RICHMOND', 'CENTRAL'}\n",
      "Missing values in PdDistrict: 0\n",
      "\n",
      "Resolution:\n",
      "0              NONE\n",
      "1    ARREST, BOOKED\n",
      "2              NONE\n",
      "Name: Resolution, dtype: object\n",
      "\n",
      "Address:\n",
      "0         FOLSOM ST / 23RD ST\n",
      "1        200 Block of TURK ST\n",
      "2    700 Block of HARRISON ST\n",
      "Name: Address, dtype: object\n",
      "Number of blocks: 615322\n",
      "Number of corners: 260404\n",
      "Number of neither: 0\n",
      "Number of both: 0\n",
      "\n",
      "X and Y:\n",
      "Empty X coords :0\n",
      "Empty Y coords :0\n",
      "Distinct X coords :34243\n",
      "Distinct Y coords :34243\n",
      "Distinct X,Y pairs :34243\n",
      "Distinct X coords (trim5) :10502\n",
      "Distinct Y coords (trim5):9284\n",
      "Distinct X,Y pairs (trim5):28696\n",
      "Distinct X coords (trim4) :1387\n",
      "Distinct Y coords (trim4):1018\n",
      "Distinct X,Y pairs (trim4):28646\n",
      "Distinct X coords (trim3) :147\n",
      "Distinct Y coords (trim3):107\n",
      "Distinct X,Y pairs (trim3):8859\n",
      "Distinct X coords (trim2) :17\n",
      "Distinct Y coords (trim2):13\n",
      "Distinct X,Y pairs (trim2):139\n",
      "Distinct X coords (trim1) :3\n",
      "Distinct Y coords (trim1):3\n",
      "Distinct X,Y pairs (trim1):5\n"
     ]
    }
   ],
   "source": [
    "print (\"Exploring data, columns, and formats\")\n",
    "print (\"\\nColumns:\")\n",
    "# View all columns\n",
    "print (df_train.columns)\n",
    "\n",
    "# Explore 'Dates'\n",
    "print (\"\\nDates:\")\n",
    "print (df_train['Dates'].head(3))\n",
    "## Add time of day, day of week, weekend, week of year, season of year, holiday\n",
    "## Add early morning, morning, afternoon, early evening, evening, late evening\n",
    "## 0500-0800, 0800-1100, 1100-1600, 1600-2100, 2100-0200, 0200-0500\n",
    "\n",
    "# Explore 'Category'\n",
    "print (\"\\nCategory:\")\n",
    "print (df_train['Category'].head(3))\n",
    "print (set(df_train['Category']))\n",
    "## OUTCOME VARIABLE\n",
    "## Possible issue with 'Trea' and 'Trespass'\n",
    "## Convert 'TREA' to 'TRESPASS'\n",
    "len(df_train[df_train['Category'] == 'TREA'])\n",
    "\n",
    "# Explore 'Descript'\n",
    "print (\"\\nDescript:\")\n",
    "print (df_train['Descript'].head(3))\n",
    "## NOT IN TEST DATA\n",
    "## Does not appear to be useful\n",
    "\n",
    "# Explore 'DayOfWeek'\n",
    "print (\"\\nDayOfWeek:\")\n",
    "print (df_train['DayOfWeek'].head(3))\n",
    "print (set(df_train['DayOfWeek']))\n",
    "total = 0\n",
    "for x in set(df_train['DayOfWeek']):\n",
    "    total += len(df_train[df_train['DayOfWeek'] == x])\n",
    "print (\"Missing values in DayOfWeek: \" + str(len(df_train) - total))\n",
    "## Looks good\n",
    "\n",
    "# Explore 'PdDistrict'\n",
    "print (\"\\nPdDistrict:\")\n",
    "print (df_train['PdDistrict'].head(3))\n",
    "print (set(df_train['PdDistrict']))\n",
    "total = 0\n",
    "for x in set(df_train['PdDistrict']):\n",
    "    total += len(df_train[df_train['PdDistrict'] == x])\n",
    "print (\"Missing values in PdDistrict: \" + str(len(df_train) - total))\n",
    "## Looks good\n",
    "\n",
    "# Explore 'Resolution'\n",
    "print (\"\\nResolution:\")\n",
    "print (df_train['Resolution'].head(3))\n",
    "## NOT IN TEST DATA\n",
    "## Does not appear to be useful\n",
    "\n",
    "# Explore 'Address'\n",
    "print (\"\\nAddress:\")\n",
    "print (df_train['Address'].head(3))\n",
    "## Count Blocks, Corners\n",
    "blocks = corners = neither = 0\n",
    "for address in df_train['Address']:\n",
    "    if \"block\" in address.lower():\n",
    "        blocks += 1\n",
    "    elif \"/\" in address:\n",
    "        corners += 1\n",
    "    else:\n",
    "        neither += 1\n",
    "print (\"Number of blocks: \" + str(blocks))\n",
    "print (\"Number of corners: \" + str(corners))\n",
    "print (\"Number of neither: \" + str(neither))\n",
    "print (\"Number of both: \" + str(blocks + corners - (len(df_train) - neither)))\n",
    "## Add block or corner\n",
    "\n",
    "# Explore 'X' and 'Y'\n",
    "print (\"\\nX and Y:\")\n",
    "print (\"Empty X coords :\" + str(len(df_train[df_train['X'] == 0])))\n",
    "print (\"Empty Y coords :\" + str(len(df_train[df_train['Y'] == 0])))\n",
    "print (\"Distinct X coords :\" + str(len(set(df_train['X']))))\n",
    "print (\"Distinct Y coords :\" + str(len(set(df_train['Y']))))\n",
    "## Form coordinate\n",
    "df_train['XY'] = df_train['X']**2 + df_train['Y']**2\n",
    "print (\"Distinct X,Y pairs :\" + str(len(set(df_train['XY']))))\n",
    "## Trim\n",
    "df_train['Ytrim5'] = df_train['Y'].apply(lambda x: round(x,5))\n",
    "df_train['Xtrim5'] = df_train['X'].apply(lambda x: round(x,5))\n",
    "df_train['XYtrim5'] = df_train['Xtrim5']**2 + df_train['Ytrim5']**2\n",
    "print (\"Distinct X coords (trim5) :\" + str(len(set(df_train['Xtrim5']))))\n",
    "print (\"Distinct Y coords (trim5):\" + str(len(set(df_train['Ytrim5']))))\n",
    "print (\"Distinct X,Y pairs (trim5):\" + str(len(set(df_train['XYtrim5']))))\n",
    "df_train['Ytrim4'] = df_train['Y'].apply(lambda x: round(x,4))\n",
    "df_train['Xtrim4'] = df_train['X'].apply(lambda x: round(x,4))\n",
    "df_train['XYtrim4'] = df_train['Xtrim4']**2 + df_train['Ytrim4']**2\n",
    "print (\"Distinct X coords (trim4) :\" + str(len(set(df_train['Xtrim4']))))\n",
    "print (\"Distinct Y coords (trim4):\" + str(len(set(df_train['Ytrim4']))))\n",
    "print (\"Distinct X,Y pairs (trim4):\" + str(len(set(df_train['XYtrim4']))))\n",
    "df_train['Ytrim3'] = df_train['Y'].apply(lambda x: round(x,3))\n",
    "df_train['Xtrim3'] = df_train['X'].apply(lambda x: round(x,3))\n",
    "df_train['XYtrim3'] = df_train['Xtrim3']**2 + df_train['Ytrim3']**2\n",
    "print (\"Distinct X coords (trim3) :\" + str(len(set(df_train['Xtrim3']))))\n",
    "print (\"Distinct Y coords (trim3):\" + str(len(set(df_train['Ytrim3']))))\n",
    "print (\"Distinct X,Y pairs (trim3):\" + str(len(set(df_train['XYtrim3']))))\n",
    "df_train['Ytrim2'] = df_train['Y'].apply(lambda x: round(x,2))\n",
    "df_train['Xtrim2'] = df_train['X'].apply(lambda x: round(x,2))\n",
    "df_train['XYtrim2'] = df_train['Xtrim2']**2 + df_train['Ytrim2']**2\n",
    "print (\"Distinct X coords (trim2) :\" + str(len(set(df_train['Xtrim2']))))\n",
    "print (\"Distinct Y coords (trim2):\" + str(len(set(df_train['Ytrim2']))))\n",
    "print (\"Distinct X,Y pairs (trim2):\" + str(len(set(df_train['XYtrim2']))))\n",
    "df_train['Ytrim1'] = df_train['Y'].apply(lambda x: round(x,1))\n",
    "df_train['Xtrim1'] = df_train['X'].apply(lambda x: round(x,1))\n",
    "df_train['XYtrim1'] = df_train['Xtrim1']**2 + df_train['Ytrim1']**2\n",
    "print (\"Distinct X coords (trim1) :\" + str(len(set(df_train['Xtrim1']))))\n",
    "print (\"Distinct Y coords (trim1):\" + str(len(set(df_train['Ytrim1']))))\n",
    "print (\"Distinct X,Y pairs (trim1):\" + str(len(set(df_train['XYtrim1']))))\n",
    "## Drop X, Y individuals\n",
    "df_train = df_train.drop(['X', 'Y', \\\n",
    "                          'Ytrim1', 'Xtrim1', \\\n",
    "                          'Ytrim2', 'Xtrim2', \\\n",
    "                          'Ytrim3', 'Xtrim3', \\\n",
    "                          'Ytrim4', 'Xtrim4', \\\n",
    "                          'Ytrim5', 'Xtrim5', \\\n",
    "                          'XYtrim5', 'XYtrim4', \\\n",
    "                          'XYtrim3', 'XYtrim1', \\\n",
    "                          'XY'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "- Add euclidean distance using X,Y\n",
    "- Add different size 'plots' of land for X,Y (trimming)\n",
    "- Add temperature, precipitation\n",
    "- Add block or corner\n",
    "- Add time of day, week of year, season of year, holiday, sun up or down\n",
    "- Add early morning, morning, afternoon, early evening, evening, late evening\n",
    "- - 0500-0800, 0800-1100, 1100-1600, 1600-2100, 2100-0200, 0200-0500\n",
    "- Convert 'TREA' to 'TRESPASS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize with dummy variables\n",
    "#dummies_df_train = df_train[['DayOfWeek', 'PdDistrict']]\n",
    "#dummies_df_train = pandas.get_dummies(dummies_df_train)\n",
    "#dummies_df_test = df_test[['DayOfWeek', 'PdDistrict']]\n",
    "#dummies_df_test = pandas.get_dummies(dummies_df_test)\n",
    "\n",
    "# Format Dates\n",
    "## Hour of day\n",
    "df_train['Hour'] = pandas.to_datetime(df_train['Dates']).dt.hour\n",
    "df_test['Hour'] = pandas.to_datetime(df_test['Dates']).dt.hour\n",
    "\n",
    "## Time of day\n",
    "\n",
    "df_train['Time'] = df_train['Hour'].apply(lambda x: DetermineTime(x))\n",
    "df_test['Time'] = df_test['Hour'].apply(lambda x: DetermineTime(x))\n",
    "\n",
    "## Day\n",
    "df_train['DayTime'] = df_train['Hour'].apply(lambda x: DetermineDayTime(x))\n",
    "df_test['DayTime'] = df_test['Hour'].apply(lambda x: DetermineDayTime(x))\n",
    "\n",
    "## Week of year\n",
    "df_train['Week'] = pandas.to_datetime(df_train['Dates']).dt.week\n",
    "df_test['Week'] = pandas.to_datetime(df_test['Dates']).dt.week\n",
    "\n",
    "## Season of year\n",
    "#df_train['Season'] = pandas.to_datetime(df_train['Dates']).dt.week\n",
    "#df_test['Season'] = pandas.to_datetime(df_test['Dates']).dt.week\n",
    "\n",
    "## Reduce to date\n",
    "df_train['Date'] = pandas.to_datetime(df_train['Dates']).dt.date\n",
    "df_test['Date'] = pandas.to_datetime(df_test['Dates']).dt.date\n",
    "\n",
    "## Holidays\n",
    "cal = holidaysCalendar()\n",
    "holidays = cal.holidays(start=pandas.to_datetime(df_train['Date']).min(), end=pandas.to_datetime(df_train['Date']).max())\n",
    "df_train['Holiday'] = (pandas.to_datetime(df_train['Date'])).dt.date.isin(holidays)\n",
    "df_test['Holiday'] = (pandas.to_datetime(df_test['Date'])).dt.date.isin(holidays)\n",
    "\n",
    "# Format Coords\n",
    "## Trim 1-5\n",
    "## Create euclidean distance\n",
    "### CREATED IN INITIAL EDA FOR TRAIN ###\n",
    "## Form coordinate\n",
    "df_test['XY'] = df_test['X']**2 + df_test['Y']**2\n",
    "## Trim\n",
    "\n",
    "df_test['Ytrim5'] = df_test['Y'].apply(lambda x: round(x,5))\n",
    "df_test['Xtrim5'] = df_test['X'].apply(lambda x: round(x,5))\n",
    "df_test['XYtrim5'] = df_test['Xtrim5']**2 + df_test['Ytrim5']**2\n",
    "df_test['Ytrim4'] = df_test['Y'].apply(lambda x: round(x,4))\n",
    "df_test['Xtrim4'] = df_test['X'].apply(lambda x: round(x,4))\n",
    "df_test['XYtrim4'] = df_test['Xtrim4']**2 + df_test['Ytrim4']**2\n",
    "df_test['Ytrim3'] = df_test['Y'].apply(lambda x: round(x,3))\n",
    "df_test['Xtrim3'] = df_test['X'].apply(lambda x: round(x,3))\n",
    "df_test['XYtrim3'] = df_test['Xtrim3']**2 + df_test['Ytrim3']**2\n",
    "df_test['Ytrim2'] = df_test['Y'].apply(lambda x: round(x,2))\n",
    "df_test['Xtrim2'] = df_test['X'].apply(lambda x: round(x,2))\n",
    "df_test['XYtrim2'] = df_test['Xtrim2']**2 + df_test['Ytrim2']**2\n",
    "df_test['Ytrim1'] = df_test['Y'].apply(lambda x: round(x,1))\n",
    "df_test['Xtrim1'] = df_test['X'].apply(lambda x: round(x,1))\n",
    "df_test['XYtrim1'] = df_test['Xtrim1']**2 + df_test['Ytrim1']**2\n",
    "\n",
    "## Drop X, Y individuals\n",
    "df_test = df_test.drop(['X', 'Y', \\\n",
    "                          'Ytrim1', 'Xtrim1', \\\n",
    "                          'Ytrim2', 'Xtrim2', \\\n",
    "                          'Ytrim3', 'Xtrim3', \\\n",
    "                          'Ytrim4', 'Xtrim4', \\\n",
    "                          'Ytrim5', 'Xtrim5', \\\n",
    "                          'XYtrim5', 'XYtrim4', \\\n",
    "                          'XYtrim3', 'XYtrim1', \\\n",
    "                          'XY'],axis=1)\n",
    "# Format Address\n",
    "## Add address type\n",
    "df_train['LocationType'] = df_train['Address'].apply(lambda x: DetermineLocationType(x))\n",
    "df_test['LocationType'] = df_test['Address'].apply(lambda x: DetermineLocationType(x))\n",
    "# Add Temperature\n",
    "## Create client\n",
    "#ow = openweather.OpenWeather()\n",
    "\n",
    "## Find weather stations closest\n",
    "#df_train['X'].mean\n",
    "#df_train['Y'].mean\n",
    "#stations = ow.find_stations_near(-122.425892, 37.774599, 1000)\n",
    "#print (stations)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_train = df_train.drop(['Date', 'Dates', 'Address', 'Resolution', 'Descript'],axis=1)\n",
    "df_test = df_test.drop(['Date', 'Dates', 'Address', 'Id'],axis=1)\n",
    "\n",
    "train_data = df_train\n",
    "test_data = df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode time\n",
    "train_data = train_data.join(pandas.get_dummies(train_data['Time'])).drop('Time', axis=1)\n",
    "test_data = test_data.join(pandas.get_dummies(test_data['Time'])).drop('Time', axis=1)\n",
    "# Encode LocationType\n",
    "train_data = train_data.join(pandas.get_dummies(train_data['LocationType'])).drop('LocationType', axis=1)\n",
    "test_data = test_data.join(pandas.get_dummies(test_data['LocationType'])).drop('LocationType', axis=1)\n",
    "# Encode PdDistrict\n",
    "train_data = train_data.join(pandas.get_dummies(train_data['PdDistrict'])).drop('PdDistrict', axis=1)\n",
    "test_data = test_data.join(pandas.get_dummies(test_data['PdDistrict'])).drop('PdDistrict', axis=1) \n",
    "# Encode DayOfWeek\n",
    "train_data = train_data.join(pandas.get_dummies(train_data['DayOfWeek'])).drop('DayOfWeek', axis=1)\n",
    "test_data = test_data.join(pandas.get_dummies(test_data['DayOfWeek'])).drop('DayOfWeek', axis=1)\n",
    "# Encode XY\n",
    "## Re-trim\n",
    "train_data['XYtrim2'] = train_data['XYtrim2'].apply(lambda x: round(x,0))\n",
    "test_data['XYtrim2'] = test_data['XYtrim2'].apply(lambda x: round(x,0))\n",
    "train_data = train_data.join(pandas.get_dummies(train_data['XYtrim2'])).drop('XYtrim2', axis=1)\n",
    "test_data = test_data.join(pandas.get_dummies(test_data['XYtrim2'])).drop('XYtrim2', axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([  'Category',       'Hour',    'DayTime',       'Week',    'Holiday',\n",
       "        'afternoon',     'earlyE',     'earlyM',    'evening',      'lateE',\n",
       "          'morning',      'block',     'corner',    'BAYVIEW',    'CENTRAL',\n",
       "        'INGLESIDE',    'MISSION',   'NORTHERN',       'PARK',   'RICHMOND',\n",
       "         'SOUTHERN',    'TARAVAL', 'TENDERLOIN',     'Friday',     'Monday',\n",
       "         'Saturday',     'Sunday',   'Thursday',    'Tuesday',  'Wednesday',\n",
       "            16398.0,      16399.0,      16400.0,      16401.0,      16402.0,\n",
       "            16403.0,      16404.0,      16405.0,      16406.0,      16407.0,\n",
       "            16408.0,      16409.0,      16410.0,      16411.0,      16412.0,\n",
       "            16413.0,      16414.0,      16415.0,      16416.0,      16417.0,\n",
       "            16418.0,      16419.0,      16420.0,      16421.0,      16422.0,\n",
       "            16423.0,      16424.0,      16425.0,      16426.0,      16427.0,\n",
       "            16428.0,      16429.0,      16430.0,      16431.0,      16432.0,\n",
       "            16433.0,      16434.0,      16435.0,      16436.0,      22620.0],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([  'Category',       'Hour',    'DayTime',       'Week',    'Holiday',\n",
       "        'afternoon',     'earlyE',     'earlyM',    'evening',      'lateE',\n",
       "          'morning',      'block',     'corner',    'BAYVIEW',    'CENTRAL',\n",
       "        'INGLESIDE',    'MISSION',   'NORTHERN',       'PARK',   'RICHMOND',\n",
       "         'SOUTHERN',    'TARAVAL', 'TENDERLOIN',     'Friday',     'Monday',\n",
       "         'Saturday',     'Sunday',   'Thursday',    'Tuesday',  'Wednesday',\n",
       "            16398.0,      16399.0,      16400.0,      16401.0,      16402.0,\n",
       "            16403.0,      16404.0,      16405.0,      16406.0,      16407.0,\n",
       "            16408.0,      16409.0,      16410.0,      16411.0,      16412.0,\n",
       "            16413.0,      16414.0,      16415.0,      16416.0,      16417.0,\n",
       "            16418.0,      16419.0,      16420.0,      16421.0,      16422.0,\n",
       "            16423.0,      16424.0,      16425.0,      16426.0,      16427.0,\n",
       "            16428.0,      16429.0,      16430.0,      16431.0,      16432.0,\n",
       "            16433.0,      16434.0,      16435.0,      16436.0,      22620.0],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "for column in train_data.columns:\n",
    "    if train_data[column].dtype == type(object):\n",
    "        train_data[column] = le.fit_transform(train_data[column])\n",
    "for column in test_data.columns:\n",
    "    if test_data[column].dtype == type(object):\n",
    "        test_data[column] = le.fit_transform(test_data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into data/labels, train/dev\n",
    "train_labels = train_data['Category'][:-50000]\n",
    "dev_labels = train_data['Category'][-50000:]\n",
    "\n",
    "train_data = train_data[train_data.columns.difference(['Category'])][:-50000]\n",
    "dev_data = train_data[train_data.columns.difference(['Category'])][-50000:]\n",
    "\n",
    "\n",
    "# Labels list\n",
    "labels = list(set(train_labels))\n",
    "train_labels = train_labels.apply(lambda x: labels.index(x))\n",
    "dev_labels = dev_labels.apply(lambda x: labels.index(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "##### K-Nearest Neighbor with k=50\n",
    "\n",
    "Need to add GridSearchCV here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = {'n_neighbors': range(5,50, 5)}\n",
    "\n",
    "neigh = GridSearchCV(KNeighborsClassifier(), parameters)\n",
    "neigh.fit(train_data, train_labels)\n",
    "print (sorted(neigh.cv_results_.keys()))\n",
    "\n",
    "# Train KNN Classifier\n",
    "#neigh = KNeighborsClassifier(n_neighbors=15)\n",
    "\n",
    "# Fit\n",
    "#neigh.fit(train_data, train_labels) \n",
    "\n",
    "# Score\n",
    "#print (neigh.score(dev_data, dev_labels))\n",
    "\n",
    "#if WRITE:\n",
    "#    PredictAndPrint(test_data, neigh, \"output_knn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "##### Final Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10686\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "\n",
    "#parameters = {'n_estimators': range(10,50, 10), 'min_samples_split': range(2,5, 1)}\n",
    "#rfc = GridSearchCV(RandomForestClassifier(), parameters)\n",
    "#rfc.fit(train_data, train_labels)\n",
    "#print (sorted(rfc.cv_results_.keys()))\n",
    "\n",
    "# Train RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=3, random_state=0)\n",
    "\n",
    "# Fit\n",
    "rfc.fit(train_data, train_labels)\n",
    "\n",
    "# Score\n",
    "print (rfc.score(dev_data, dev_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "##### Final Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "##### Final Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Predict\\nrfc_result = rfc.predict(test_data)\\n\\n# Output\\nrfc_result = pandas.DataFrame(rfc_result)\\nrfc_result = pandas.get_dummies(rfc_result, prefix=\\'\\', prefix_sep=\\'\\')\\n# Add null categories to make kaggle happy\\nrfc_result = rfc_result.T.reindex(header).T.fillna(0)\\nrfc_result.to_csv(\"output_rfc.csv\", compression=\\'gzip\\', chunksize=1000)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Predict\n",
    "rfc_result = rfc.predict(test_data)\n",
    "\n",
    "# Output\n",
    "rfc_result = pandas.DataFrame(rfc_result)\n",
    "rfc_result = pandas.get_dummies(rfc_result, prefix='', prefix_sep='')\n",
    "# Add null categories to make kaggle happy\n",
    "rfc_result = rfc_result.T.reindex(header).T.fillna(0)\n",
    "rfc_result.to_csv(\"output_rfc.csv\", compression='gzip', chunksize=1000)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural-Network\n",
    "##### Final Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "39\n",
      "825726\n"
     ]
    }
   ],
   "source": [
    "numFeatures = len(train_data.columns)\n",
    "numClasses = len(labels)\n",
    "numSamples = len(train_data)\n",
    "#numTestExamples = len(dev_data)\n",
    "\n",
    "print (numFeatures)\n",
    "print (numClasses)\n",
    "print (numSamples)\n",
    "#print (numTestExamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "825726/825726 [==============================] - 50s 61us/step - loss: 101476.3327 - acc: 0.0870\n",
      "Epoch 2/3\n",
      "825726/825726 [==============================] - 49s 60us/step - loss: 101451.5280 - acc: 0.1973\n",
      "Epoch 3/3\n",
      "825726/825726 [==============================] - 49s 60us/step - loss: 101451.4842 - acc: 0.1971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a0d7d6438>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, ActivityRegularization\n",
    "from keras.layers import Flatten, Reshape\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import optimizers\n",
    "\n",
    "# All parameter gradients will be clipped to\n",
    "# a maximum value of 0.5 and\n",
    "# a minimum value of -0.5.\n",
    "sgd = optimizers.SGD(lr=0.01, clipvalue=.5)\n",
    "\n",
    "# Define\n",
    "sequential_model = Sequential()\n",
    "sequential_model.add(Dense(units=numFeatures * 2, activation='relu', input_shape=(numFeatures,)))\n",
    "sequential_model.add(Dense(units=int(numFeatures), activation='softmax' ))\n",
    "#sequential_model.add(Dropout(0.05, noise_shape=None, seed=None))\n",
    "sequential_model.add(ActivityRegularization(l1=1000.0, l2=1000.0))\n",
    "\n",
    "# Compile\n",
    "sequential_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "# Fit\n",
    "sequential_model.fit(train_data.values, train_labels.values, epochs=3, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "825726/825726 [==============================] - 62s 75us/step - loss: 37598.2966 - acc: 0.0127\n",
      "Epoch 2/3\n",
      "825726/825726 [==============================] - 64s 78us/step - loss: 36154.1876 - acc: 0.0131\n",
      "Epoch 3/3\n",
      "825726/825726 [==============================] - 64s 77us/step - loss: 36082.4805 - acc: 0.0125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a7918ef98>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define\n",
    "sequential_model = Sequential()\n",
    "sequential_model.add(Dense(units=numFeatures * 4, activation='relu', input_shape=(numFeatures,)))\n",
    "sequential_model.add(Dense(units=int(numFeatures * 2), activation='softmax' ))\n",
    "sequential_model.add(Dense(units=int(numFeatures)))\n",
    "#sequential_model.add(Dropout(0.05, noise_shape=None, seed=None))\n",
    "sequential_model.add(ActivityRegularization(l1=1000.0, l2=1000.0))\n",
    "\n",
    "# Compile\n",
    "sequential_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "# Fit\n",
    "sequential_model.fit(train_data.values, train_labels.values, epochs=3, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "825726/825726 [==============================] - 52s 63us/step - loss: 200004.5186 - acc: 0.0294\n",
      "Epoch 2/3\n",
      "825726/825726 [==============================] - 52s 63us/step - loss: 200010.9851 - acc: 0.0294\n",
      "Epoch 3/3\n",
      "825726/825726 [==============================] - 52s 63us/step - loss: 200010.9850 - acc: 0.0294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a01a63e10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define\n",
    "sequential_model = Sequential()\n",
    "sequential_model.add(Dense(units=numFeatures * 2, activation='relu', input_shape=(numFeatures,)))\n",
    "sequential_model.add(Dense(units=int(numFeatures), activation='softmax' ))\n",
    "#sequential_model.add(Dropout(0.05, noise_shape=None, seed=None))\n",
    "sequential_model.add(ActivityRegularization(l1=1000.0, l2=1000.0))\n",
    "\n",
    "# Compile\n",
    "sequential_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "# Fit\n",
    "sequential_model.fit(train_data.values, train_labels.values, epochs=3, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "825726/825726 [==============================] - 64s 77us/step - loss: nan - acc: 0.0017\n",
      "Epoch 2/3\n",
      "825726/825726 [==============================] - 63s 77us/step - loss: nan - acc: 0.0017\n",
      "Epoch 3/3\n",
      "825726/825726 [==============================] - 63s 77us/step - loss: nan - acc: 0.0017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a02352f98>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define\n",
    "sequential_model = Sequential()\n",
    "sequential_model.add(Dense(units=numFeatures * 4, activation='relu', input_shape=(numFeatures,)))\n",
    "sequential_model.add(Dense(units=int(numFeatures * 2), activation='softmax' ))\n",
    "sequential_model.add(Dense(units=int(numFeatures)))\n",
    "#sequential_model.add(Dropout(0.05, noise_shape=None, seed=None))\n",
    "sequential_model.add(ActivityRegularization(l1=1000.0, l2=1000.0))\n",
    "\n",
    "# Compile\n",
    "sequential_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "# Fit\n",
    "sequential_model.fit(train_data.values, train_labels.values, epochs=3, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "825726/825726 [==============================] - 64s 78us/step - loss: nan - acc: 0.0018\n",
      "Epoch 2/3\n",
      "825726/825726 [==============================] - 64s 77us/step - loss: nan - acc: 0.0017\n",
      "Epoch 3/3\n",
      "825726/825726 [==============================] - 64s 78us/step - loss: nan - acc: 0.0017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a02d06e48>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define\n",
    "sequential_model = Sequential()\n",
    "sequential_model.add(Dense(units=numFeatures * 4, activation='sigmoid', input_shape=(numFeatures,)))\n",
    "sequential_model.add(Dense(units=int(numFeatures * 2), activation='softmax' ))\n",
    "sequential_model.add(Dense(units=int(numFeatures)))\n",
    "#sequential_model.add(Dropout(0.05, noise_shape=None, seed=None))\n",
    "sequential_model.add(ActivityRegularization(l1=1000.0, l2=1000.0))\n",
    "\n",
    "# Compile\n",
    "sequential_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "# Fit\n",
    "sequential_model.fit(train_data.values, train_labels.values, epochs=3, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "825726/825726 [==============================] - 65s 78us/step - loss: nan - acc: 0.0017\n",
      "Epoch 2/3\n",
      "825726/825726 [==============================] - 64s 78us/step - loss: nan - acc: 0.0017\n",
      "Epoch 3/3\n",
      "825726/825726 [==============================] - 64s 78us/step - loss: nan - acc: 0.0017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a01a524a8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define\n",
    "sequential_model = Sequential()\n",
    "sequential_model.add(Dense(units=numFeatures * 4, activation='sigmoid', input_shape=(numFeatures,)))\n",
    "sequential_model.add(Dense(units=int(numFeatures * 2), activation='softmax' ))\n",
    "sequential_model.add(Dense(units=int(numFeatures)))\n",
    "#sequential_model.add(Dropout(0.05, noise_shape=None, seed=None))\n",
    "sequential_model.add(ActivityRegularization(l1=10.0, l2=10.0))\n",
    "\n",
    "# Compile\n",
    "sequential_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "# Fit\n",
    "sequential_model.fit(train_data.values, train_labels.values, epochs=3, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "825726/825726 [==============================] - 67s 81us/step - loss: 56.4259 - acc: 0.0689\n",
      "Epoch 2/3\n",
      "825726/825726 [==============================] - 66s 80us/step - loss: 50.7646 - acc: 0.0653\n",
      "Epoch 3/3\n",
      "825726/825726 [==============================] - 67s 81us/step - loss: 57.1263 - acc: 0.0581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a0ae5ae10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define\n",
    "sequential_model = Sequential()\n",
    "sequential_model.add(Dense(units=numFeatures * 4, activation='sigmoid', input_shape=(numFeatures,)))\n",
    "sequential_model.add(Dense(units=int(numFeatures * 2), activation='softmax' ))\n",
    "sequential_model.add(Dense(units=int(numFeatures)))\n",
    "#sequential_model.add(Dropout(0.05, noise_shape=None, seed=None))\n",
    "sequential_model.add(ActivityRegularization(l1=10.0, l2=10.0))\n",
    "\n",
    "# Compile\n",
    "sequential_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# Fit\n",
    "sequential_model.fit(train_data.values, train_labels.values, epochs=3, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "825726/825726 [==============================] - 67s 81us/step - loss: 68.7225 - acc: 0.0622\n",
      "Epoch 2/3\n",
      "825726/825726 [==============================] - 67s 81us/step - loss: 52.6472 - acc: 0.0607\n",
      "Epoch 3/3\n",
      "825726/825726 [==============================] - 67s 81us/step - loss: 53.1067 - acc: 0.0634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a0b31cfd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define\n",
    "sequential_model = Sequential()\n",
    "sequential_model.add(Dense(units=numFeatures * 4, activation='relu', input_shape=(numFeatures,)))\n",
    "sequential_model.add(Dense(units=int(numFeatures * 2), activation='softmax' ))\n",
    "sequential_model.add(Dense(units=int(numFeatures)))\n",
    "#sequential_model.add(Dropout(0.05, noise_shape=None, seed=None))\n",
    "sequential_model.add(ActivityRegularization(l1=10.0, l2=10.0))\n",
    "\n",
    "# Compile\n",
    "sequential_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# Fit\n",
    "sequential_model.fit(train_data.values, train_labels.values, epochs=3, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "825726/825726 [==============================] - 67s 82us/step - loss: 2102.1361 - acc: 0.0158\n",
      "Epoch 2/3\n",
      "825726/825726 [==============================] - 67s 81us/step - loss: 1381.8686 - acc: 0.0245\n",
      "Epoch 3/3\n",
      "825726/825726 [==============================] - 67s 81us/step - loss: 1474.8053 - acc: 0.0380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a0b982b70>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define\n",
    "sequential_model = Sequential()\n",
    "sequential_model.add(Dense(units=numFeatures * 4, activation='relu', input_shape=(numFeatures,)))\n",
    "sequential_model.add(Dense(units=int(numFeatures * 2), activation='softmax' ))\n",
    "sequential_model.add(Dense(units=int(numFeatures)))\n",
    "#sequential_model.add(Dropout(0.05, noise_shape=None, seed=None))\n",
    "sequential_model.add(ActivityRegularization(l1=1000.0, l2=1000.0))\n",
    "\n",
    "# Compile\n",
    "sequential_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# Fit\n",
    "sequential_model.fit(train_data.values, train_labels.values, epochs=3, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "sequential_model.evaluate(dev_data, dev_labels, batch_size=128)\n",
    "\n",
    "# Predict\n",
    "keras_seq_result = sequential_model.predict(test_data, batch_size=128)\n",
    "\n",
    "# Output\n",
    "keras_seq_result = pandas.DataFrame(keras_seq_result)\n",
    "keras_seq_result = pandas.get_dummies(keras_seq_result, prefix='', prefix_sep='')\n",
    "# Add null categories to make kaggle happy\n",
    "keras_seq_result = keras_seq_result.T.reindex(header).T.fillna(0)\n",
    "keras_seq_result.to_csv(\"output_keras_seq.csv\", compression='gzip', chunksize=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Parameters\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Constants\n",
    "testX = tf.constant(dev_data.values, dtype=tf.float32)\n",
    "hiddenlayer1_size = 2\n",
    "hiddenlayer2_size = 1\n",
    "miniBatchSize = 1\n",
    "\n",
    "# placeholders\n",
    "x_ = tf.placeholder(tf.float32, shape=[None, numFeatures], name='x')\n",
    "y_ = tf.placeholder(tf.int32, shape=[None], name='y')\n",
    "\n",
    "# and Variables\n",
    "w1 = tf.get_variable('w1', shape=[numFeatures, hiddenlayer1_size])\n",
    "b1 = tf.get_variable('b1', shape=[hiddenlayer1_size])\n",
    "w2 = tf.get_variable('w2', shape=[hiddenlayer1_size, numClasses])\n",
    "b2 = tf.get_variable('b3', shape=[numClasses])\n",
    "\n",
    "\n",
    "# (2) Model\n",
    "def model(input_layer):\n",
    "    hidden_layer1 = tf.nn.sigmoid(tf.matmul(input_layer, w1) + b1)\n",
    "    output_layer = tf.nn.softmax(tf.matmul(hidden_layer1, w2) + b2)\n",
    "    return output_layer\n",
    "\n",
    "# (2) Model\n",
    "def model_r(input_layer):\n",
    "    hidden_layer1 = tf.nn.relu(tf.matmul(input_layer, w1) + b1)\n",
    "    output_layer = tf.nn.softmax(tf.matmul(hidden_layer1, w2) + b2)\n",
    "    return output_layer\n",
    "    \n",
    "\n",
    "# (3) Cost\n",
    "def cost(data, labels):\n",
    "    cc = tf.sqrt(tf.square(labels - model(data)))\n",
    "    return  cc\n",
    "\n",
    "# (4) Ojbective (and solver)\n",
    "y_one_hot = tf.one_hot(y_, numClasses)\n",
    "cc = cost(x_, y_one_hot)\n",
    "gd = tf.train.GradientDescentOptimizer(0.1)\n",
    "step = gd.minimize(cc)\n",
    "test_preds = model(testX)\n",
    "test_preds_r = model_r(testX)\n",
    "output = \"\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cost_vec = []\n",
    "    cost_vec_r = []\n",
    "    for i in range(15):\n",
    "        print (i)\n",
    "        for start, end in zip(range(0, numSamples, miniBatchSize), range(miniBatchSize, numSamples, miniBatchSize)):\n",
    "            batch = train_data.values[start:end], train_labels[start:end]\n",
    "            _, cost, test__preds_r = sess.run([step, cc, test_preds_r], feed_dict={x_: batch[0], y_: batch[1]})\n",
    "    \n",
    "    prediction=tf.argmax(test_preds_r,axis=1)\n",
    "    output = prediction.eval(feed_dict={x_: test_data.values})\n",
    "    print (\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
