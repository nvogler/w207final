{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project a dataset containing crime statistics in the San Francisco area from X to Y is analyzed with the goal of creating a model to predict the nature of a crime based only on its time and location. \n",
    "\n",
    "More information on the project and its dataset can be found here:\n",
    "https://www.kaggle.com/c/sf-crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as holidaysCalendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header (from sample submission)\n",
    "header = [\"ID\", \"ARSON\", \"ASSAULT\", \"BAD CHECKS\", \"BRIBERY\", \"BURGLARY\",\n",
    "          \"DISORDERLY CONDUCT\", \"DRIVING UNDER THE INFLUENCE\", \"DRUG/NARCOTIC\",\n",
    "          \"DRUNKENNESS\", \"EMBEZZLEMENT\", \"EXTORTION\", \"FAMILY OFFENSES\",\n",
    "          \"FORGERY/COUNTERFEITING\", \"FRAUD\", \"GAMBLING\", \"KIDNAPPING\", \"LARCENY/THEFT\",\n",
    "          \"LIQUOR LAWS\", \"LOITERING\", \"MISSING PERSON\", \"NON-CRIMINAL\", \"OTHER OFFENSES\",\n",
    "          \"PORNOGRAPHY/OBSCENE MAT\", \"PROSTITUTION\", \"RECOVERED VEHICLE\", \"ROBBERY\",\n",
    "          \"RUNAWAY\", \"SECONDARY CODES\", \"SEX OFFENSES FORCIBLE\", \"SEX OFFENSES NON FORCIBLE\",\n",
    "          \"STOLEN PROPERTY\", \"SUICIDE\", \"SUSPICIOUS OCC\", \"TREA\", \"TRESPASS\", \"VANDALISM\",\n",
    "          \"VEHICLE THEFT\", \"WARRANTS\", \"WEAPON LAWS\"]\n",
    "\n",
    "# Load data\n",
    "df_train = pandas.read_csv(\"train.csv\")\n",
    "df_test = pandas.read_csv(\"test.csv\")\n",
    "\n",
    "# Drop duplicates\n",
    "df_train = df_train.drop_duplicates()\n",
    "df_test = df_test.drop_duplicates()\n",
    "\n",
    "# Binarize with dummy variables\n",
    "dummies_df_train = df_train[['DayOfWeek', 'PdDistrict']]\n",
    "dummies_df_train = pandas.get_dummies(dummies_df_train)\n",
    "dummies_df_test = df_test[['DayOfWeek', 'PdDistrict']]\n",
    "dummies_df_test = pandas.get_dummies(dummies_df_test)\n",
    "\n",
    "# Format Dates\n",
    "## Time of day\n",
    "df_train['Time'] = pandas.to_datetime(df_train['Dates']).dt.hour\n",
    "df_test['Time'] = pandas.to_datetime(df_test['Dates']).dt.hour\n",
    "## Week of year\n",
    "df_train['Week'] = pandas.to_datetime(df_train['Dates']).dt.week\n",
    "df_test['Week'] = pandas.to_datetime(df_test['Dates']).dt.week\n",
    "## Holidays\n",
    "#cal = holidaysCalendar()\n",
    "#holidays = cal.holidays(start=pandas.to_datetime(df['Dates']).min(), end=pandas.to_datetime(df['Dates']).max())\n",
    "#df['Holiday'] = pandas.to_datetime(df['Dates']).isin(holidays)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_train = df_train.drop(['Address', 'Resolution', 'Descript', 'Dates', 'DayOfWeek', 'PdDistrict'],axis=1)\n",
    "df_test = df_test.drop(['Address', 'Dates', 'DayOfWeek', 'PdDistrict'],axis=1)\n",
    "\n",
    "# Format Coords\n",
    "df_train['Y'] = df_train['Y'].apply(lambda x: round(x,4))\n",
    "df_train['X'] = df_train['X'].apply(lambda x: round(x,4))\n",
    "#df['Coords'] = df['X'].astype(str) + \", \" + df['Y'].astype(str)\n",
    "df_test['Y'] = df_test['Y'].apply(lambda x: round(x,4))\n",
    "df_test['X'] = df_test['X'].apply(lambda x: round(x,4))\n",
    "#df['Coords'] = df['X'].astype(str) + \", \" + df['Y'].astype(str)\n",
    "\n",
    "# Join\n",
    "parsed_df_train = df_train.join(dummies_df_train)\n",
    "parsed_df_test = df_test.join(dummies_df_test)\n",
    "\n",
    "# Split into data/labels, train/dev\n",
    "train_data = parsed_df_train[parsed_df_train.columns.difference(['Category'])][:50000]\n",
    "train_labels = parsed_df_train['Category'][:50000]\n",
    "\n",
    "dev_data = parsed_df_train[parsed_df_train.columns.difference(['Category'])][-5000:]\n",
    "dev_labels = parsed_df_train['Category'][-5000:]\n",
    "\n",
    "# Labels list\n",
    "labels = list(set(train_labels))\n",
    "train_labels = train_labels.apply(lambda x: labels.index(x))\n",
    "dev_labels = dev_labels.apply(lambda x: labels.index(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "##### K-Nearest Neighbor with k=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# Train KNN Classifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=50)\n",
    "\n",
    "# Fit\n",
    "neigh.fit(train_data, train_labels) \n",
    "\n",
    "# Score\n",
    "neigh.score(dev_data, dev_labels)\n",
    "\n",
    "# Predict\n",
    "neigh_result = neigh.predict(test_data)\n",
    "\n",
    "# Output\n",
    "neigh_result = pandas.DataFrame(neigh_result)\n",
    "neigh_result = pandas.get_dummies(neigh_result, prefix='', prefix_sep='')\n",
    "# Add null categories to make kaggle happy\n",
    "neigh_result = neigh_result.T.reindex(header).T.fillna(0)\n",
    "neigh_result.to_csv(\"output_knn.csv\", compression='gzip', chunksize=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "##### Final Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "##### Final Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "##### Final Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "\n",
    "# Fit\n",
    "rfc.fit(train_data, train_labels)\n",
    "\n",
    "# Score\n",
    "rfc.score(dev_data, dev_labels)\n",
    "\n",
    "# Predict\n",
    "rfc_result = rfc.predict(test_data)\n",
    "\n",
    "# Output\n",
    "rfc_result = pandas.DataFrame(rfc_result)\n",
    "rfc_result = pandas.get_dummies(rfc_result, prefix='', prefix_sep='')\n",
    "# Add null categories to make kaggle happy\n",
    "rfc_result = rfc_result.T.reindex(header).T.fillna(0)\n",
    "rfc_result.to_csv(\"output_rfc.csv\", compression='gzip', chunksize=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural-Network\n",
    "##### Final Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "numFeatures = len(train_data.columns)\n",
    "numClasses = len(labels)\n",
    "numSamples = len(train_data)\n",
    "numTestExamples = len(dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 50000 into shape (21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-bb11c6301107>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0msequential_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# Score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 50000 into shape (21)"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten, Reshape\n",
    "\n",
    "# Define\n",
    "sequential_model = Sequential()\n",
    "sequential_model.add(Dense(units=64, activation='relu', input_shape=(numFeatures,)))\n",
    "## model.add(Dense(32, input_shape=(16,)))\n",
    "## Now the model will take as input arrays of shape (*, 16)\n",
    "## and output arrays of shape (*, 32)\n",
    "sequential_model.add(Dense(units=10, activation='softmax'))\n",
    "## After the first layer, you don't need to specify\n",
    "## the size of the input anymore\n",
    "# Compile\n",
    "sequential_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit\n",
    "sequential_model.fit(train_data.values.reshape(-1,21), train_labels.values.reshape(-1,21), epochs=10, batch_size=100)\n",
    "\n",
    "# Score\n",
    "sequential_model.evaluate(dev_data, dev_labels, batch_size=128)\n",
    "\n",
    "# Predict\n",
    "keras_seq_result = sequential_model.predict(test_data, batch_size=128)\n",
    "\n",
    "# Output\n",
    "keras_seq_result = pandas.DataFrame(keras_seq_result)\n",
    "keras_seq_result = pandas.get_dummies(keras_seq_result, prefix='', prefix_sep='')\n",
    "# Add null categories to make kaggle happy\n",
    "keras_seq_result = keras_seq_result.T.reindex(header).T.fillna(0)\n",
    "keras_seq_result.to_csv(\"output_keras_seq.csv\", compression='gzip', chunksize=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define\n",
    "dense_model = Sequential([\n",
    "    Dense(32, input_dim=2),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "# Compile\n",
    "dense_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit\n",
    "dense_model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
    "\n",
    "# Score\n",
    "dense_model.evaluate(x_test, y_test, batch_size=128)\n",
    "\n",
    "# Predict\n",
    "classes = dense_model.predict(x_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Parameters\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Constants\n",
    "testX = tf.constant(dev_data.values, dtype=tf.float32)\n",
    "hiddenlayer1_size = 2\n",
    "hiddenlayer2_size = 1\n",
    "miniBatchSize = 1\n",
    "\n",
    "# placeholders\n",
    "x_ = tf.placeholder(tf.float32, shape=[None, numFeatures], name='x')\n",
    "y_ = tf.placeholder(tf.int32, shape=[None], name='y')\n",
    "\n",
    "# and Variables\n",
    "w1 = tf.get_variable('w1', shape=[numFeatures, hiddenlayer1_size])\n",
    "b1 = tf.get_variable('b1', shape=[hiddenlayer1_size])\n",
    "w2 = tf.get_variable('w2', shape=[hiddenlayer1_size, numClasses])\n",
    "b2 = tf.get_variable('b3', shape=[numClasses])\n",
    "\n",
    "\n",
    "# (2) Model\n",
    "def model(input_layer):\n",
    "    hidden_layer1 = tf.nn.sigmoid(tf.matmul(input_layer, w1) + b1)\n",
    "    output_layer = tf.nn.softmax(tf.matmul(hidden_layer1, w2) + b2)\n",
    "    return output_layer\n",
    "\n",
    "# (2) Model\n",
    "def model_r(input_layer):\n",
    "    hidden_layer1 = tf.nn.relu(tf.matmul(input_layer, w1) + b1)\n",
    "    output_layer = tf.nn.softmax(tf.matmul(hidden_layer1, w2) + b2)\n",
    "    return output_layer\n",
    "    \n",
    "\n",
    "# (3) Cost\n",
    "def cost(data, labels):\n",
    "    cc = tf.sqrt(tf.square(labels - model(data)))\n",
    "    return  cc\n",
    "\n",
    "# (4) Ojbective (and solver)\n",
    "y_one_hot = tf.one_hot(y_, numClasses)\n",
    "cc = cost(x_, y_one_hot)\n",
    "gd = tf.train.GradientDescentOptimizer(0.1)\n",
    "step = gd.minimize(cc)\n",
    "test_preds = model(testX)\n",
    "test_preds_r = model_r(testX)\n",
    "output = \"\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cost_vec = []\n",
    "    cost_vec_r = []\n",
    "    for i in range(15):\n",
    "        print (i)\n",
    "        for start, end in zip(range(0, numSamples, miniBatchSize), range(miniBatchSize, numSamples, miniBatchSize)):\n",
    "            batch = train_data.values[start:end], train_labels[start:end]\n",
    "            _, cost, test__preds_r = sess.run([step, cc, test_preds_r], feed_dict={x_: batch[0], y_: batch[1]})\n",
    "    \n",
    "    prediction=tf.argmax(test_preds_r,axis=1)\n",
    "    output = prediction.eval(feed_dict={x_: test_data.values})\n",
    "    print (\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
